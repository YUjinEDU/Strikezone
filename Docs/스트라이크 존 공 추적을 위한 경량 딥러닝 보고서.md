# Lightweight Deep Learning for AR Strike Zone Ball TrackingAR 스트라이크 존 공 추적을 위한 경량 딥러닝

## Introduction소개

Building a low-cost, single-camera _Augmented Reality (AR) Strike Zone_ system for baseball involves accurately detecting a fast-moving, small ball and determining if it passes through a virtual strike zone. The current system uses classical computer vision (color filtering and a pinhole camera model) to estimate 3D coordinates and make ball/strike judgments in real time[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=My%20first%20approach%20was%20to,go%2C%20and%20they%20move%20fast). However, color and shape-based detection can struggle with varying backgrounds, lighting, and motion blur[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=Press%20enter%20or%20click%20to,view%20image%20in%20full%20size)[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=I%20also%20looked%20at%20Hough,create%20circles%20out%20of%20them). To improve robustness and accuracy, we investigate integrating a **lightweight deep learning** model for ball detection and tracking, while maintaining real-time performance on low-end hardware. This report explores: (1) suitable lightweight object detection models (e.g. YOLOv8n, MobileNet-SSD, EfficientDet-Lite) and their performance in resource-constrained, real-time scenarios, (2) strategies for training on small, fast-moving objects (data augmentation and available datasets), (3) techniques for optimizing models with TensorRT/ONNX/TFLite for deployment on Android or low-power PCs, (4) methods to combine deep learning detection with the existing **ArUco marker** coordinate system and pinhole camera 3D depth pipeline, and (5) an assessment of the solution’s novelty, utility, and reproducibility from an academic perspective. We also compare two deployment architectures: running inference **on-device (phone only)** versus **offloading video to a PC** for processing, analyzing trade-offs in latency and feasibility.저비용 단일 카메라 증강현실(AR) 스트라이크 존 시스템을 야구에 적용하려면 빠르게 움직이고 작은 공을 정확하게 감지하고 가상 스트라이크 존을 통과하는지 여부를 판단해야 합니다. 현재 시스템은 고전적인 컴퓨터 비전(색상 필터링 및 핀홀 카메라 모델)을 사용해 3D 좌표를 추정하고 실시간으로 볼/스트라이크 판정을 내리고 있습니다. 그러나 색상 및 형태 기반 검출은 배경, 조명 변화 및 모션 블러에 취약할 수 있습니다. 견고성과 정확도를 개선하기 위해 경량 딥러닝 모델을 볼 검출 및 추적에 통합하면서 저사양 하드웨어에서 실시간 성능을 유지하는 것을 조사합니다.이 보고서는 다음을 탐구합니다: (1) 리소스가 제한된 실시간 환경에 적합한 경량 객체 검출 모델(예: YOLOv8n, MobileNet-SSD, EfficientDet-Lite)과 그 성능, (2) 작은 고속 물체에 대한 학습 전략(데이터 증강 및 활용 가능한 데이터셋), (3) Android 또는 저전력 PC 배포를 위한 TensorRT/ONNX/TFLite로 모델 최적화하는 기법, (4) 딥러닝 검출을 기존 ArUco 마커 좌표계 및 핀홀 카메라 3D 깊이 파이프라인과 결합하는 방법, 및 (5) 학술적 관점에서 솔루션의 새로움, 유용성 및 재현 가능성 평가.또한 두 가지 배포 아키텍처를 비교합니다: 장치 내 추론(휴대폰 단독) 실행 versus 비디오를 PC로 오프로드하여 처리하는 방식으로, 지연시간 및 실현 가능성 측면의 트레이드오프를 분석합니다.

## Lightweight Models for Real-Time Ball Detection실시간 공 검출을 위한 경량 모델

Real-time ball tracking requires object detectors that are both fast and accurate on small objects. **YOLO (You Only Look Once)** models are known for high speed and have been applied successfully to track fast sports objects like hockey pucks or soccer balls[viam.com](https://www.viam.com/post/guide-yolo-model-real-time-object-detection-with-examples#:~:text=Sports%20analysis). The newest YOLOv8 family offers a **“nano” model (YOLOv8n)** specifically designed for edge devices, as does the **MobileNet-SSD** architecture (e.g. SSD with MobileNet v2/v3 backbone) and Google’s **EfficientDet-Lite** models optimized for TFLite. We compare these options:실시간 공 추적에는 빠르면서도 작은 물체에 대해 정확한 객체 검출기가 필요합니다. YOLO(You Only Look Once) 모델은 속도가 빠르기로 알려져 있으며 하키 퍽이나 축구공과 같은 빠른 스포츠 물체 추적에 성공적으로 적용되어 왔습니다. 최신 YOLOv8 계열은 엣지 기기를 위해 특별히 설계된 "나노" 모델(YOLOv8n)을 제공하며, MobileNet-SSD 아키텍처(예: MobileNet v2/v3 백본을 사용하는 SSD)와 구글의 TFLite에 최적화된 EfficientDet-Lite 모델들도 마찬가지입니다. 우리는 이러한 옵션들을 비교합니다:

- **YOLOv8n:** Ultralytics’ YOLOv8n is around 6 MB in size[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=Size%20) and achieves a strong balance of accuracy and speed. In a recent study (for pedestrians on CPU), YOLOv8n had the highest detection accuracy (mAP) while still running at ~5.9 FPS on a mobile-class CPU[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=FPS)[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=YOLOv8n). It outperformed other lightweight models, proving _“fast enough for real-time use”_ with significantly better precision[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=YOLOv8n%20gave%20the%20best%20results,speed%20and%20accuracy%20are%20important). YOLO’s modern design (PAFPN, multi-scale features, etc.) improves small object detection relative to older one-stage detectors[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Small%20Object%20Detection). For example, YOLOv5/v8 models were noted to handle small objects better than MobileNet-SSD, which _“drops on small object detection”_ without tuning[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=1.%20Approximately%2028%20%E2%80%93%2033,YOLOv5s%2C%20v8s)[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Small%20Object%20Detection). Actual sports applications confirm YOLO’s suitability: one project fine-tuned YOLOv8 to detect baseballs in broadcast footage, finding its speed and context-awareness ideal for the task[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=For%20those%20unfamiliar%20with%20machine,identify%20objects%20within%20a%20frame). YOLO’s one-pass design yields high FPS, making it popular for **sports analytics** where quick object localization is critical[viam.com](https://www.viam.com/post/guide-yolo-model-real-time-object-detection-with-examples#:~:text=The%20key%20advantage%20of%20YOLO,streams%20quicker%20than%20other%20models)[viam.com](https://www.viam.com/post/guide-yolo-model-real-time-object-detection-with-examples#:~:text=This%20makes%20it%20effective%20for,monitoring%2C%20sports%20analytics%2C%20and%20surveillance).YOLOv8n: Ultralytics의 YOLOv8n은 크기가 약 6MB이며 정확도와 속도 사이에서 강한 균형을 이룹니다. 최근 연구(모바일급 CPU에서 보행자 탐지 기준)에서 YOLOv8n은 약 5.9 FPS로 실행되는 동안 가장 높은 탐지 정확도(mAP)를 기록했습니다7universum.com7universum.com. 이는 다른 경량 모델들을 능가했으며, 실시간 사용에 "충분히 빠르다"고 판단될 만큼 훨씬 나은 정밀도를 보여주었습니다7universum.com. YOLO의 현대적 설계(PAFPN, 다중 스케일 특징 등)는 구형 단일 단계 검출기들에 비해 작은 물체 검출을 개선합니다cytron.io. 예를 들어, YOLOv5/v8 모델들은 MobileNet-SSD보다 작은 물체를 더 잘 처리하는 것으로 언급되었는데, MobileNet-SSD는 튜닝 없이 작은 물체 검출에서 "성능이 떨어진다"고 합니다cytron.io. 실제 스포츠 응용에서도 YOLO의 적합성이 확인되었는데, 한 프로젝트는 방송 영상에서 야구공을 탐지하도록 YOLOv8을 미세조정하여 속도와 문맥 인식이 해당 작업에 이상적이라는 결론을 얻었습니다medium.com. YOLO의 일회 통과 설계는 높은 FPS를 제공하여 빠른 물체 위치 파악이 중요한 스포츠 분석에서 인기를 끌고 있습니다viam.comviam.com.
    
- **MobileNet-SSD:** SSD (Single Shot Detector) with a MobileNet backbone is a classic lightweight model intended for real-time operation on mobile CPUs. Model size (~5–6 MB) is small[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=Size%20)[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Model%20Size), and when quantized and run with TensorFlow Lite it can exceed 10 FPS on devices like a Raspberry Pi 4[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Inference%20Speed%20). In a direct comparison, an unoptimized MobileNet-SSD achieved **~70 FPS** on CPU but with very low accuracy (in a pedestrian test, it _“missed most of the targets”_ and produced many false detections)[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=As%20we%20can%20see%2C%20YOLOv8n,detections%2C%20like%20confusing%20trees%20with)[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=MobileNet,is%20retrained%20with%20better%20data). This highlights a trade-off: MobileNet-SSD is blazingly fast but often _underperforms on small, fast objects_ unless carefully retrained with ample data[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=MobileNet,is%20retrained%20with%20better%20data). SSD’s multi-scale feature maps were an early attempt to handle various object sizes, but newer models have surpassed it. For instance, one study noted MobileNet-SSD’s accuracy on small objects lagged behind newer YOLO models (COCO mAP ~20–25% vs. ~28–33% for YOLOv5n)[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=1.%20Approximately%2028%20%E2%80%93%2033,YOLOv5s%2C%20v8s). Nonetheless, if speed is the top priority and the model can be heavily fine-tuned (or if using an accelerator), MobileNet-SSD can be viable. There are real-world examples of MobileNet-SSD on mobile devices for object tracking, but for a tiny, fast ball, we anticipate needing extensive augmentation to improve its detection rate.MobileNet-SSD: MobileNet 백본을 사용하는 SSD(Single Shot Detector)는 모바일 CPU에서 실시간 동작을 목표로 한 고전적인 경량 모델입니다. 모델 크기(~5–6 MB)는 작으며7universum.comcytron.io, 양자화하여 TensorFlow Lite로 실행하면 Raspberry Pi 4와 같은 장치에서 10 FPS를 넘길 수 있습니다cytron.io. 직접 비교에서 최적화되지 않은 MobileNet-SSD는 CPU에서 약 70 FPS를 기록했지만 정확도는 매우 낮았습니다(보행자 테스트에서 “대부분의 대상물을 놓쳤고” 많은 잘못된 검출을 생성했습니다)7universum.com7universum.com.이는 트레이드오프를 강조합니다: MobileNet-SSD는 매우 빠르지만 충분한 데이터로 신중하게 재학습하지 않으면 작은 빠른 물체에서는 종종 성능이 떨어집니다7universum.com. SSD의 다중 스케일 특징 맵은 다양한 객체 크기를 처리하려는 초기 시도였지만, 최신 모델들이 이를 능가했습니다. 예를 들어 한 연구에서는 작은 객체에 대한 MobileNet-SSD의 정확도가 최신 YOLO 모델들보다 뒤처진다고 지적했으며(COCO mAP 약 20–25% 대 YOLOv5n의 약 28–33%)cytron.io. 그럼에도 불구하고 속도가 최우선이고 모델을 대폭 미세조정할 수 있거나(또는 가속기를 사용하는 경우) MobileNet-SSD는 실용적일 수 있습니다.모바일 장치에서 객체 추적을 위해 MobileNet-SSD를 사용한 실제 사례들이 있지만, 아주 작고 빠른 공을 위해서는 검출률을 높이기 위해 광범위한 증강이 필요할 것으로 예상됩니다.
    
- **EfficientDet-Lite:** This family (Lite0 to Lite3, etc.) is Google’s mobile-optimized detector using EfficientNet backbones and a custom PAN/FPN. EfficientDet-Lite models are available as pre-trained TFLite models and strike a balance between MobileNet-SSD and YOLO in accuracy. For example, EfficientDet-Lite0 (quantized) had “middle-of-the-road” performance in one benchmark – higher accuracy than SSD in some cases but slower (fourth place in speed out of five models tested)[ejtech.io](https://www.ejtech.io/learn/tflite-object-detection-model-comparison#:~:text=match%20at%20L292%20EfficientDet,FPNLite%20on%20the%20Edge%20TPU)[ejtech.io](https://www.ejtech.io/learn/tflite-object-detection-model-comparison#:~:text=EfficientDet,FPNLite%20on%20the%20Edge%20TPU). It achieved decent mAP (~60% on a test set) but did not excel at very small objects without custom training[ejtech.io](https://www.ejtech.io/learn/tflite-object-detection-model-comparison#:~:text=EfficientDet,FPNLite%20on%20the%20Edge%20TPU). The advantage is that EfficientDet-Lite models are **designed for TFLite** and thus easy to deploy on Android; they also scale up if needed (Lite1–Lite2 for more accuracy at cost of speed). In mobile apps, EfficientDet-Lite0 can run in real-time (e.g. ~20–30 FPS on modern phone CPUs for 320×320 inputs) with integer quantization, according to Google’s demos[blog.tensorflow.org](https://blog.tensorflow.org/2020/03/higher-accuracy-on-vision-models-with-efficientnet-lite.html#:~:text=Higher%20accuracy%20on%20vision%20models,time%20%28e.g.)[arxiv.org](https://arxiv.org/html/2409.16808v1#:~:text=,optimized%20performance%20on%20the). It may provide better out-of-the-box accuracy on small objects than an untuned MobileNet-SSD, thanks to improved feature fusion.EfficientDet-Lite: 이 계열(Lite0부터 Lite3 등)은 EfficientNet 백본과 맞춤형 PAN/FPN을 사용하는 구글의 모바일 최적화 객체 탐지기입니다. EfficientDet-Lite 모델은 사전학습된 TFLite 모델로 제공되며 정확도 측면에서 MobileNet-SSD와 YOLO의 중간 균형을 이룹니다. 예를 들어 EfficientDet-Lite0(양자화된)는 한 벤치마크에서 “보통 수준”의 성능을 보였는데—어떤 경우에는 SSD보다 높은 정확도를 보였지만 속도는 더 느렸습니다(테스트한 다섯 모델 중 속도에서 네 번째). 약간의 mAP(테스트 세트에서 약 60%)를 달성했으나 맞춤 훈련 없이 매우 작은 객체에서는 뛰어나지 않았습니다. 장점은 EfficientDet-Lite 모델이 TFLite용으로 설계되어 안드로이드에 배포하기 쉽고 필요하면 확장할 수 있다는 점(Lite1–Lite2는 속도를 희생하고 정확도를 높임)입니다. 모바일 앱에서는 EfficientDet-Lite0이 정수 양자화로 실시간 실행이 가능하며(예: 320×320 입력에 대해 최신 휴대폰 CPU에서 약 20–30 FPS), 구글 데모에 따르면 튜닝되지 않은 MobileNet-SSD보다 향상된 특징 융합 덕분에 바로 사용할 때 작은 객체에 대해 더 나은 정확도를 제공할 수 있습니다.
    
- **Other models:** New ultra-light models like **NanoDet** (an anchor-free edge detector) and YOLOv5/YOLOv7 “Tiny” variants are also options. A 2025 study comparing YOLOv8n, NanoDet-Plus, and MobileNet-SSD for edge devices found YOLOv8n was most accurate with acceptable speed, NanoDet was smallest (memory footprint ~4.7 MB) but slightly slower, and MobileNet-SSD was fastest but **unacceptably inaccurate** without retraining[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=As%20we%20can%20see%2C%20YOLOv8n,detections%2C%20like%20confusing%20trees%20with)[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=MobileNet,unless%20retrained%20with%20better%20data). This reinforces that YOLOv8n (or similar) is likely the top choice when both **precision and speed** matter for ball detection, whereas Mobilenet-SSD would need significant improvement in detection quality to be useful.다른 모델: NanoDet(앵커 프리 엣지 탐지기)와 YOLOv5/YOLOv7의 "Tiny" 변형과 같은 새로운 초경량 모델들도 선택지에 포함됩니다. 2025년의 한 연구에서는 엣지 디바이스용으로 YOLOv8n, NanoDet-Plus, MobileNet-SSD를 비교했는데, YOLOv8n이 허용 가능한 속도에서 가장 정확했고, NanoDet는 가장 작았으며(메모리 풋프린트 약 4.7MB) 다소 느렸고, MobileNet-SSD는 가장 빨랐지만 재학습 없이 사용하기에는 허용할 수 없을 정도로 부정확하다고 밝혔습니다7universum.com7universum.com. 이는 공 검출에서 정밀도와 속도가 모두 중요할 경우 YOLOv8n(또는 유사 모델)이 최선의 선택일 가능성이 높음을 뒷받침하며, 반면 MobileNet-SSD는 유용하려면 탐지 품질 면에서 상당한 개선이 필요함을 의미합니다.
    

**Performance Trade-offs:** In summary, YOLOv8n offers the best _accuracy-to-speed_ ratio for ball detection, being used successfully in sports tracking scenarios. It may run at ~5–15 FPS on a mobile CPU (depending on input resolution) and much faster with GPU or accelerator assistance[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Inference%20Speed%20). MobileNet-SSD can run at higher FPS even on CPU (30+ FPS) but at the cost of missing many small, fast targets unless optimized[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=MobileNet,is%20retrained%20with%20better%20data). EfficientDet-Lite sits in between, with better accuracy than SSD and TFLite-ready deployment at the cost of some speed. Notably, all these models are relatively small (5–6 MB) and can be further quantized to reduce memory. Real use cases show YOLO models tracking 150 km/h baseball pitches and other sports balls in real time, which attests to their capability when properly trained[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=For%20those%20unfamiliar%20with%20machine,identify%20objects%20within%20a%20frame)[viam.com](https://www.viam.com/post/guide-yolo-model-real-time-object-detection-with-examples#:~:text=Sports%20analysis).성능 절충: 요약하자면, YOLOv8n은 공 탐지에 대해 정확도와 속도 비율이 가장 우수하여 스포츠 추적 시나리오에서 성공적으로 사용되고 있다. 모바일 CPU에서는 입력 해상도에 따라 대략 초당 5–15프레임으로 실행될 수 있으며, GPU나 가속기 지원이 있으면 훨씬 더 빠르게 동작한다cytron.io. MobileNet-SSD는 CPU에서도 더 높은 프레임률(30+ FPS)로 실행될 수 있지만, 최적화하지 않으면 많은 작고 빠른 대상들을 놓치는 대가를 치른다7universum.com. EfficientDet-Lite는 그 중간에 위치하며, SSD보다 좋은 정확도를 제공하고 TFLite 준비된 배포가 가능하지만 속도 측면에서 일부 손해가 있다. 특히 이들 모델은 모두 비교적 작아(5–6 MB) 추가 양자화를 통해 메모리를 더 줄일 수 있다. 실제 사용 사례에서는 YOLO 모델이 150 km/h의 야구 투구 및 다른 스포츠 공을 실시간으로 추적하는 것으로 나타나며, 이는 적절히 학습되었을 때의 성능을 입증한다medium.comviam.com.

## Data Augmentation and Datasets for Small/Fast Objects소형/고속 물체를 위한 데이터 증강 및 데이터셋

**Detecting a baseball** in flight is challenging because the object is small (only a few pixels in many frames), often motion-blurred, and sometimes camouflaged against complex backgrounds (crowds, uniforms). Special training data strategies are needed to make a deep model robust in this scenario:비행 중인 야구공을 감지하는 것은 물체가 작고(많은 프레임에서 몇 픽셀에 불과), 종종 모션 블러가 발생하며 때로는 복잡한 배경(관중, 유니폼)과 섞여 위장되어 있기 때문에 어렵습니다. 이러한 상황에서 딥 모델을 견고하게 만들기 위해서는 특수한 학습 데이터 전략이 필요합니다:

- **High-Resolution Training:** A key strategy is to train with a larger input size so that small objects are better represented. Researchers found that increasing YOLO’s input from the standard 608×608 to **1024×1024** was necessary to detect sports balls; at 608 resolution many balls “became invisible or almost invisible” when resized[pdfs.semanticscholar.org](https://pdfs.semanticscholar.org/6926/10f36e339a631ab2bf920e72ca796387567a.pdf#:~:text=Since%20the%20goal%20of%20detecting,objects%20invisible%20or%20almost%20invisible). Using 1024px (or higher) allows the network’s grid to have finer granularity on small objects. The trade-off is slower inference, but with a nano model it may be manageable. For our case, if real-time at full HD is required, one compromise is to run the model on a scaled-down frame (e.g. 720p) but ensure the model was trained on high-res crops of the ball so it has learned the features.고해상도 학습: 핵심 전략은 입력 크기를 키워 작은 물체가 더 잘 표현되도록 학습하는 것이다. 연구자들은 표준 608×608에서 1024×1024로 YOLO의 입력을 늘려야 스포츠 공을 감지할 수 있음을 발견했는데; 608 해상도에서는 많은 공들이 크기 조정 시 “보이지 않거나 거의 보이지 않게” 되었다. pdfs.semanticscholar.org. 1024px(또는 그 이상)를 사용하면 네트워크의 그리드가 작은 물체에 대해 더 세밀한 해상도를 가지게 된다. 단점은 추론 속도가 느려진다는 것이지만, 나노 모델이라면 관리 가능한 수준일 수 있다. 우리의 경우 전체 HD에서 실시간 처리가 필요하다면 한 가지 타협안으로 프레임을 축소된 크기(예: 720p)로 모델을 실행하되, 모델이 공의 고해상도 크롭으로 학습되어 공의 특징을 학습했는지 확인하는 방법이 있다.
    
- **Augmentation Techniques:** Augmentations can greatly aid in learning small, fast object features. Common techniques include random **image scaling/cropping**, which effectively creates examples of the object at different sizes. YOLO’s default **mosaic augmentation** (which stitches multiple images) can help expose the model to balls at various scales and contexts in one training batch. **Brightness/contrast adjustments** are important due to outdoor lighting changes[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=Roboflow%20offers%20the%20option%20to,in%20a%20standard%20baseball%20broadcast). In one case, a developer used Roboflow to auto-adjust contrast and vary brightness, exposure, and noise on baseball images[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=Roboflow%20offers%20the%20option%20to,in%20a%20standard%20baseball%20broadcast). This helped the model handle different stadium lighting conditions (day, night, shadows). **Background augmentation** is also useful: e.g. overlaying ball images on random backgrounds (copy-paste augmentation) to simulate varied scenes and prevent overfitting to one field or camera angle. Crucially, to handle motion blur, one should include artificially **blurred** examples of the ball. A practitioner tracking cricket balls with YOLOv8 mentioned _“training the model on blurry balls”_ as a brute-force approach to improve detection when the ball is moving fast[reddit.com](https://www.reddit.com/r/computervision/comments/1bi7f1x/estimating_cricket_ball_speed_and_trajectory_with/#:~:text=%E2%80%A2%20%202y%20ago). This implies generating motion-blurred versions of ball images (using Photoshop or OpenCV filters) or simply using frames where the ball is naturally blurred as training data. Rotation augmentation (random orientations) isn’t as relevant for roughly spherical balls, but could help if the seam pattern needs to be detected (usually not necessary for just localization). Basic flips were used in prior sports ball experiments: one group doubled their dataset by horizontal flipping, and even tried vertical flipping (though the latter created unphysical scenarios, it further augmented the data)[pdfs.semanticscholar.org](https://pdfs.semanticscholar.org/6926/10f36e339a631ab2bf920e72ca796387567a.pdf#:~:text=In%20the%20YPB%2B%20model%2C%20the,as%20well%20which%20results%20in). While vertical flips may not make sense physically, horizontal flips are valid (a left-handed pitcher vs right-handed, etc., the trajectory mirrored).증강 기법: 증강은 작고 빠른 물체의 특징을 학습하는 데 크게 도움이 될 수 있습니다. 일반적인 기법으로는 무작위 이미지 스케일링/크롭이 있으며, 이는 물체를 다양한 크기로 포함한 예제를 효과적으로 생성합니다. YOLO의 기본 모자이크 증강(여러 이미지를 이어 붙이는 방식)은 하나의 학습 배치에서 다양한 스케일과 문맥으로 공을 노출시키는 데 도움이 될 수 있습니다. 밝기/대비 조정은 야외 조명 변화 때문에 중요합니다(medium.com). 한 사례에서는 개발자가 Roboflow를 사용해 야구 이미지의 대비를 자동 조정하고 밝기, 노출, 노이즈를 다양화했다고 합니다(medium.com). 이는 낮, 밤, 그림자 등 다양한 경기장 조명 조건을 모델이 처리하는 데 도움이 되었습니다. 배경 증강도 유용한데, 예를 들어 공 이미지를 무작위 배경에 오버레이하는(복사-붙여넣기 증강) 방식은 다양한 장면을 시뮬레이션하고 특정 필드나 카메라 각도에 과적합되는 것을 막습니다. 무엇보다 모션 블러를 처리하려면 공이 인위적으로 블러 처리된 예제를 포함해야 합니다. 크리켓 공을 YOLOv8로 추적한 실무자는 공이 빠르게 움직일 때 검출 성능을 향상시키기 위한 강력한 방법으로 “블러 처리된 공으로 모델을 훈련”하는 것을 언급했습니다(reddit.com).이는 포토샵이나 OpenCV 필터를 사용해 공 이미지의 모션 블러 버전을 생성하거나, 단순히 공이 자연스럽게 블러된 프레임을 훈련 데이터로 사용하는 것을 의미합니다. 회전 증강(무작위 방향)은 대체로 구형에 가까운 공에는 그다지 관련성이 없지만, 이음새 패턴을 감지해야 하는 경우에는 도움이 될 수 있습니다(단순 위치 검출에는 보통 필요하지 않습니다). 이전 스포츠 공 실험에서는 기본적인 뒤집기 증강이 사용되었습니다: 한 그룹은 데이터셋을 수평 뒤집기로 두 배로 늘렸고, 심지어 수직 뒤집기도 시도했는데(후자는 비현실적인 시나리오를 만들었지만 데이터 증강을 더 해주었습니다)pdfs.semanticscholar.org. 수직 뒤집기는 물리적으로 말이 되지 않을 수 있지만, 수평 뒤집기는 타자나 투수의 좌우 차이(왼손 투수 대 오른손 투수 등)에서 궤적이 대칭되는 것을 고려하면 유효합니다.
    
- **Negative Samples and Hard Examples:** The dataset should include “hard negatives” – frames with no ball or where the ball is extremely small – so that the detector learns to not hallucinate false positives. In the baseball YOLO project, false positives occurred on white objects like ad boards or parts of uniform[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=The%20bounding%20boxes%2C%20while%20useful%2C,and%20catch%20these%20edge%20cases). The author mitigated this by explicitly labeling very hard-to-see balls as _“null”_ (essentially telling the model those instances are not to be detected) and by expanding the dataset to include those tricky cases[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=The%20bounding%20boxes%2C%20while%20useful%2C,and%20catch%20these%20edge%20cases). This reduces false alarms. We should similarly ensure to include frames of the empty strike zone (no ball) and perhaps other round objects (e.g. the pitcher’s white plate, catcher’s gear) labeled as background, so the model learns to distinguish the baseball.부정 샘플과 어려운 예제: 데이터셋에는 검출기가 잘못된 양성(거짓 양성)을 만들어내지 않도록 “하드 네거티브”—공이 없거나 공이 매우 작게 보이는 프레임—를 포함해야 합니다. 야구 YOLO 프로젝트에서는 광고판 같은 흰색 물체나 유니폼 일부에서 거짓 양성이 발생했습니다. 작성자는 매우 보기 힘든 공을 명시적으로 “null”(사실상 해당 인스턴스를 감지하지 말라고 모델에 알려주는 것)로 레이블링하고, 그런 까다로운 사례를 포함하도록 데이터셋을 확장하여 이 문제를 완화했습니다. 이는 오탐을 줄여줍니다. 우리도 빈 스트라이크 존(공이 없는 프레임)과 투수의 흰색 플레이트나 포수 장비 등 다른 둥근 물체들을 배경으로 레이블링하여 모델이 야구공을 구별하도록 학습시키는 것을 확실히 해야 합니다.
    
- **Open-Source Datasets:** Fortunately, there are emerging datasets for sports ball detection. The **BaseballCV** project provides open-source annotated images from baseball games. Notably, it offers a **“baseballs” dataset** with MLB broadcast frames where only baseballs are labeled[github.com](https://github.com/BaseballCV/BaseballCV#:~:text=,for%20baseballs%2C%20the%20rubber%2C%20homeplate). They list a YOLO-format dataset focusing solely on baseballs, as well as others combining ball + base/plate, etc.[github.com](https://github.com/BaseballCV/BaseballCV#:~:text=Available%20YOLO%20Pre). Using such data can bootstrap our model training. Additionally, general datasets like COCO include a “sports ball” class, but those mostly have larger balls in the image (soccer, etc.) and may not cover tiny fast baseballs well. Another resource is from research on tennis: e.g. the TrackNet dataset for tennis ball trajectories, or the dataset used in the paper by Zhao et al. which combined tennis videos (though tennis balls are larger on screen in close-up shots). If needed, we can leverage cricket ball tracking datasets or even synthetic data (simulating a ball in flight). However, for baseball, the BaseballCV’s _“MLB broadcast 15k frames”_ could be invaluable, and they even provide **15,000 unannotated frames** for custom labeling[github.com](https://github.com/BaseballCV/BaseballCV#:~:text=match%20at%20L469%20images%20that,A%20collection%20of%2010%2C000). We may use those to label our own additional training data (e.g. focusing on our camera angle). For an **AR training tool scenario (amateur baseball)**, we should also gather data from a similar single-camera setup: for instance, recording some sample pitches with a phone to capture how a ball looks in that setting (likely closer and larger than in MLB center-field camera). We might intentionally use a brightly colored ball during data collection (as done in the current system with a green ball) to create ground-truth labels, then later ensure the model generalizes back to normal white baseballs.오픈소스 데이터셋: 다행히도 스포츠 공 탐지를 위한 새 데이터셋들이 등장하고 있습니다. BaseballCV 프로젝트는 야구 경기의 주석이 달린 오픈소스 이미지를 제공합니다. 특히 MLB 중계 프레임에서 야구공만 라벨링한 “baseballs” 데이터셋을 제공합니다github.com. 그들은 야구공에만 초점을 맞춘 YOLO 형식 데이터셋뿐만 아니라 공 + 베이스/플레이트 등을 결합한 다른 데이터셋도 나열합니다github.com. 이러한 데이터를 사용하면 모델 학습을 빠르게 시작할 수 있습니다. 또한 COCO와 같은 일반 데이터셋에는 “sports ball” 클래스가 포함되어 있지만, 대부분 이미지에서 공이 더 크게 보이는 축구 등 사례가 많아 화면에서 아주 작고 빠르게 움직이는 야구공을 잘 다루지 못할 수 있습니다. 또 다른 자료로는 테니스를 대상으로 한 연구들이 있는데, 예를 들어 테니스 공 궤적을 위한 TrackNet 데이터셋이나 Zhao 등 논문에서 사용한 테니스 비디오를 결합한 데이터셋 등이 있습니다(다만 테니스 공은 클로즈업 샷에서 화면상 더 크게 보입니다). 필요하다면 크리켓 공 추적 데이터셋이나 비행 중인 공을 시뮬레이션한 합성 데이터도 활용할 수 있습니다. 그러나 야구의 경우 BaseballCV의 “MLB broadcast 15k frames”는 매우 유용할 수 있으며, 사용자 정의 라벨링을 위해 15,000개의 주석 없는 프레임도 제공한다고 합니다github.com.우리는 그런 자료들을 사용해 추가 학습용 데이터를 직접 라벨링할 수 있습니다(예: 우리 카메라 앵글에 초점을 맞춘 데이터). AR 훈련 도구 시나리오(아마추어 야구)의 경우, 유사한 단일 카메라 설정에서 데이터를 수집하는 것도 중요합니다. 예를 들어 휴대폰으로 샘플 피치를 녹화해 해당 환경에서 공이 어떻게 보이는지(MLB 중앙 카메라보다 더 가깝고 크게 보일 가능성 있음) 캡처할 수 있습니다. 데이터 수집 시 현재 시스템에서 사용한 녹색 공처럼 의도적으로 밝은 색 공을 사용해 정답 라벨을 만들고, 이후 모델이 일반 흰색 야구공으로도 일반화되도록 하는 전략을 취할 수 있습니다.
    

In summary, an effective dataset for our purpose will contain a large number of images with the ball labeled at various positions and motion states, including blur. Key augmentation strategies include increasing resolution, random cropping (to simulate zoom), brightness/contrast variation, noise, horizontal flipping, and motion blur simulation. By also incorporating open datasets (like BaseballCV’s) and ensuring to cover hard negatives, we can train a model that is _optimized for small, fast object detection_. This should markedly improve detection recall over the basic color-thresholding method, which struggled once backgrounds became non-uniform[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=The%20first%20technique%20I%20tried,time%2C%20stadium%2C%20and%20other%20factors).요약하자면, 우리의 목적에 적합한 효과적인 데이터셋은 공이 다양한 위치와 움직임 상태(블러 포함)로 라벨링된 많은 수의 이미지로 구성됩니다. 주요 증강 전략으로는 해상도 증가, 랜덤 크롭(줌 시뮬레이션), 밝기/대비 변화, 노이즈, 좌우 반전, 모션 블러 시뮬레이션 등이 있습니다. BaseballCV와 같은 공개 데이터셋을 포함하고 어려운 네거티브 사례를 충분히 포괄하면 작고 빠른 물체 검출에 최적화된 모델을 학습시킬 수 있습니다. 이는 배경이 균일하지 않을 때 어려움을 겪었던 기본 색상 임계값 방식보다 검출 재현율을 현저히 향상시킬 것입니다.

## Model Optimization for Edge Deployment (TensorRT, ONNX, TFLite)엣지 배포를 위한 모델 최적화(TensorRT, ONNX, TFLite)

Achieving real-time performance on a **low-end Windows PC or an Android phone** requires optimizing the trained model beyond the typical PyTorch/TF training format. There are several deployment toolchains available:저사양 Windows PC나 Android 휴대폰에서 실시간 성능을 달성하려면 학습된 모델을 일반적인 PyTorch/TF 학습 형식 이상으로 최적화해야 합니다. 사용할 수 있는 여러 배포 툴체인이 있습니다:

- **ONNX and TensorRT (for PC/Nvidia GPUs):** One common pipeline is to export the model to ONNX (Open Neural Network Exchange) format, then use NVIDIA’s TensorRT to optimize and run inferences. For example, the tennis-ball tracking study exported their YOLOv5 model to ONNX and then used TensorRT for quantization and acceleration[researchgate.net](https://www.researchgate.net/publication/393264089_Tennis_ball_detection_based_on_YOLOv5_with_tensorrt#:~:text=practical%20edge%20devices,pt%29%20to%20the%20ONNX). TensorRT performs low-level optimizations such as layer fusion, half or int8 precision, and GPU memory optimization, often resulting in 2-4× faster inference. In that study, introducing TensorRT significantly improved real-time performance, reducing the computation time per inference (they report a “minimum computing time of 2.28 s” for an entire process, likely involving multiple frames[researchgate.net](https://www.researchgate.net/publication/393264089_Tennis_ball_detection_based_on_YOLOv5_with_tensorrt#:~:text=efficient%20tennis%20feature%20extraction,the%20penalization%20of%20tennis%20matches), which suggests that single-frame inference was on the order of 10–20 ms after optimization). The process is typically: _export PyTorch -> ONNX -> TensorRT engine_. With YOLOv8n, Ultralytics provides built-in exporters; one can do `model.export(format="onnx")` then use TensorRT APIs or `trtexec` to compile the ONNX to a TensorRT engine. On a Windows PC with an NVIDIA GPU, this can yield very high FPS (often >100 FPS for tiny models) and ultra-low latency (a few milliseconds per frame)[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Inference%20Speed%20). Even on a CPU, ONNX Runtime can be used to get a speed boost by using all CPU threads and SSE/AVX optimizations.ONNX 및 TensorRT(PC/Nvidia GPU용): 일반적인 파이프라인 중 하나는 모델을 ONNX(Open Neural Network Exchange) 형식으로 내보낸 다음 NVIDIA의 TensorRT를 사용해 최적화하고 추론을 실행하는 것입니다. 예를 들어, 테니스 볼 추적 연구에서는 YOLOv5 모델을 ONNX로 내보내고 TensorRT를 사용해 양자화 및 가속을 수행했다고 보고했습니다researchgate.net. TensorRT는 레이어 융합, half 또는 int8 정밀도, GPU 메모리 최적화와 같은 저수준 최적화를 수행하여 종종 2–4배 빠른 추론을 가능하게 합니다. 해당 연구에서 TensorRT를 도입하면 실시간 성능이 크게 향상되어 추론당 계산 시간이 단축되었으며(전체 프로세스에 대해 “최소 계산 시간 2.28 s”를 보고했는데, 이는 아마도 여러 프레임을 포함하는 전체 과정에 대한 값이며 연구에 따르면 최적화 후 단일 프레임 추론은 대략 10–20ms 정도였음을 시사합니다researchgate.net). 일반적인 과정은 PyTorch -> ONNX -> TensorRT 엔진으로 내보내는 것입니다. YOLOv8n의 경우 Ultralytics가 내장된 익스포터를 제공하며 model.export(format="onnx")를 실행한 다음 TensorRT API나 trtexec를 사용해 ONNX를 TensorRT 엔진으로 컴파일할 수 있습니다. NVIDIA GPU가 탑재된 Windows PC에서는 매우 높은 FPS(작은 모델의 경우 종종 >100 FPS)와 초저지연(프레임당 몇 밀리초)을 얻을 수 있습니다cytron.io.심지어 CPU에서도 ONNX Runtime을 사용하여 모든 CPU 스레드와 SSE/AVX 최적화를 활용하면 속도 향상을 얻을 수 있습니다.
    
- **TensorFlow Lite (for mobile/embedded):** For Android deployment, TFLite is a natural choice. We can convert the model (if we trained in PyTorch, we might first export to ONNX or to a saved model and then use the TFLite converter). If training was done via TensorFlow/Keras (e.g. using EfficientDet-Lite or a TF OD API MobileNet-SSD), we could directly use TFLite Model Maker to quantize. TFLite allows 8-bit quantization which can dramatically speed up inference on ARM CPUs and also lets us use **Android NNAPI** or GPU delegates. For instance, a quantized SSD-MobileNet v2 model runs in ~69 ms per frame at 1280×720 on a Pixel phone (≈14 FPS) using 4 threads[ejtech.io](https://www.ejtech.io/learn/tflite-object-detection-model-comparison#:~:text=SSD,a%20slight%20reduction%20in%20speed). EfficientDet-Lite0, quantized, might run slightly slower (~80–100 ms per frame) but with better accuracy[ejtech.io](https://www.ejtech.io/learn/tflite-object-detection-model-comparison#:~:text=EfficientDet,FPNLite%20on%20the%20Edge%20TPU). YOLOv8n can also be converted to TFLite; developers have successfully run YOLOv5/8 on phones by exporting to ONNX then converting to TFLite, or by using Ultralytics’ export to TFLite directly (including a built-in NMS). It’s reported that YOLOv5s (slightly larger than v8n) could achieve ~17 FPS on an Android device with GPU delegate[docs.ultralytics.com](https://docs.ultralytics.com/compare/efficientdet-vs-yolov5/#:~:text=Comparison%20docs,2x%20faster%20while%20delivering), so YOLOv8n should be faster. Another avenue is using **NCNN**, an optimized inference engine for mobile (favored in some Chinese CV frameworks); Ultralytics also allows exporting to NCNN which can be integrated in Android apps similarly to TFLite. Additionally, **Core ML** can be used for iOS deployment – YOLOv8n has been exported to CoreML and run at >30 FPS on modern iPhones[blog.roboflow.com](https://blog.roboflow.com/mobile-object-detection-models/#:~:text=tester%20%3D%20YOLOv8CoreMLTester%28,jpg)[blog.roboflow.com](https://blog.roboflow.com/mobile-object-detection-models/#:~:text=).TensorFlow Lite(모바일/임베디드용): Android 배포의 경우 TFLite가 자연스러운 선택입니다. 모델을 변환할 수 있는데(만약 PyTorch로 학습했다면 먼저 ONNX로 내보내거나 saved model로 변환한 뒤 TFLite 변환기를 사용할 수 있습니다). TensorFlow/Keras(예: EfficientDet-Lite 또는 TF OD API의 MobileNet-SSD)를 통해 학습했다면 TFLite Model Maker를 직접 사용해 양자화할 수 있습니다. TFLite는 8비트 양자화를 지원하여 ARM CPU에서 추론 속도를 크게 높일 수 있고 Android NNAPI나 GPU delegate를 사용할 수 있게 해줍니다. 예를 들어, 양자화된 SSD-MobileNet v2 모델은 Pixel 휴대폰에서 1280×720 해상도로 프레임당 약 69ms(≈14 FPS), 4스레드를 사용하여 실행됩니다ejtech.io.EfficientDet-Lite0은 양자화된 상태에서 약간 더 느리게 실행될 수 있지만(프레임당 약 80–100ms) 더 나은 정확도를 보일 수 있습니다ejtech.io. YOLOv8n도 TFLite로 변환할 수 있으며; 개발자들은 ONNX로 내보낸 뒤 TFLite로 변환하거나 Ultralytics의 TFLite 직접 내보내기(내장 NMS 포함)를 사용하여 휴대폰에서 YOLOv5/8을 성공적으로 실행해 왔습니다. 보고에 따르면 YOLOv5s(약간 더 큰 모델)는 Android 기기에서 GPU delegate를 사용하면 약 17 FPS를 달성할 수 있으므로 YOLOv8n은 더 빠를 것으로 예상됩니다docs.ultralytics.com. 또 다른 경로로는 모바일에 최적화된 추론 엔진인 NCNN을 사용하는 방법이 있는데(일부 중국 CV 프레임워크에서 선호됨), Ultralytics도 NCNN으로의 내보내기를 허용하며 이는 TFLite와 유사하게 Android 앱에 통합할 수 있습니다.또한 iOS 배포에는 Core ML을 사용할 수 있습니다 – YOLOv8n은 CoreML로 내보내져 최신 iPhone에서 30 FPS 이상으로 실행된 사례가 있습니다blog.roboflow.comblog.roboflow.com.
    
- **Optimization Techniques:** Regardless of framework, we will apply quantization (preferably **INT8 quantization with calibration** on a representative dataset, which TensorRT and TFLite both support). This can give 2-4× speedup with minimal loss in accuracy for a well-trained model. We will also leverage **model pruning** if available (YOLOv5 had some support for pruning channels, and one could manually prune redundant channels in the trained model to shrink it further). Another approach is distilling the model or using a smaller backbone if needed. However, given YOLOv8n is already very small and our target is a single object class, we might also reduce the number of output anchors or even use a single-class model (this reduces output layer computations). In fact, when converting to TFLite, one can strip out the class dimension and have only “ball” as the class to slightly optimize.최적화 기법: 프레임워크에 관계없이 양자화(대표 데이터셋으로 보정한 INT8 양자화를 선호하며, TensorRT와 TFLite가 모두 지원)를 적용합니다. 잘 훈련된 모델의 경우 정확도 손실을 거의 없이 2–4배의 속도 향상을 제공할 수 있습니다. 가능한 경우 모델 프루닝도 활용합니다(예: YOLOv5는 채널 프루닝을 일부 지원했으며, 훈련된 모델에서 중복 채널을 수동으로 제거해 더 작게 만들 수 있습니다). 다른 접근법으로는 모델 증류나 필요 시 더 작은 백본을 사용하는 방법이 있습니다. 그러나 YOLOv8n은 이미 매우 작고 목표가 단일 객체 클래스인 점을 고려하면 출력 앵커 수를 줄이거나 단일 클래스 모델을 사용할 수도 있습니다(이는 출력 층 계산을 줄여줍니다). 실제로 TFLite로 변환할 때 클래스 차원을 제거하고 클래스가 “ball” 하나만 있도록 하면 약간의 최적화를 할 수 있습니다.
    
- **Platform-Specific Considerations:** On a Windows PC with no GPU, one can use ONNX Runtime with OpenVINO (for Intel CPUs) or even TensorRT on Jetson (if using an Nvidia Jetson Nano/Orin for edge deployment). An example from the Cytron article showed YOLOv5n running at only ~1.5–3 FPS on a Raspberry Pi 4 CPU, but when offloaded to a Coral USB TPU (Edge TPU) it jumped to ~20 FPS[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Inference%20Speed%20). So if an external accelerator is an option, it can help (e.g. the Google Coral or Intel Neural Compute Stick could run a quantized SSD or tiny YOLO model rapidly). On Android, using the NNAPI can offload to the phone’s DSP/NPU (if available) for additional speed; many Qualcomm chips have AI accelerators that TFLite can leverage. The **memory footprint** of these models (5–6 MB) is well within modern phone capabilities, and quantization can drop it to ~1.5 MB, which is excellent for embedding in an app. We must ensure the model’s input size is fixed (e.g. 416×416 or 640×640) and then scale camera frames accordingly, to make full use of the optimized inference (dynamic shapes are less optimized in TensorRT and not allowed in TFLite).플랫폼별 고려사항: GPU가 없는 Windows PC에서는 ONNX Runtime을 OpenVINO(인텔 CPU용)와 함께 사용하거나, 엣지 배포용으로 Nvidia Jetson Nano/Orin을 사용하는 경우 TensorRT를 사용할 수 있습니다. Cytron 기사에 나온 예시에서는 Raspberry Pi 4 CPU에서 YOLOv5n이 약 1.5–3 FPS로만 동작했지만, Coral USB TPU(Edge TPU)로 오프로드하자 약 20 FPS로 급증했습니다(cytron.io). 따라서 외부 가속기가 가능하다면 도움이 될 수 있습니다(예: Google Coral 또는 Intel Neural Compute Stick은 양자화된 SSD나 tiny YOLO 모델을 빠르게 실행할 수 있습니다). Android에서는 NNAPI를 사용해 가능하다면 휴대폰의 DSP/NPU로 오프로드하여 추가 속도를 얻을 수 있습니다; 많은 Qualcomm 칩에는 TFLite가 활용할 수 있는 AI 가속기가 있습니다. 이러한 모델의 메모리 점유율(5–6 MB)은 최신 휴대폰 사양에 충분히 적합하며, 양자화를 통해 약 1.5 MB로 줄일 수 있어 앱에 임베드하기에 훌륭합니다. 모델의 입력 크기(예: 416×416 또는 640×640)를 고정한 다음 카메라 프레임을 그에 맞게 스케일링하여 최적화된 추론을 최대한 활용해야 합니다(동적 형상은 TensorRT에서 덜 최적화되며 TFLite에서는 허용되지 않습니다).
    

In our integration, we will likely prototype with PyTorch on PC, then export and optimize for the target deployment. The pipeline might be: train a YOLOv8n (custom) → export to ONNX → run ONNX Runtime on PC, and separately export to TFLite (INT8) for Android. We expect on a typical Android device (mid-range 2021 phone), a quantized YOLOv8n could run ~20–30 FPS at 416×416 input, and an EfficientDet-Lite0 ~10–20 FPS at 320×320, based on reported benchmarks[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Inference%20Speed%20)[ejtech.io](https://www.ejtech.io/learn/tflite-object-detection-model-comparison#:~:text=EfficientDet,FPNLite%20on%20the%20Edge%20TPU). These are sufficient for giving nearly real-time feedback for a pitching training scenario.우리 통합에서는 PC에서 PyTorch로 프로토타입을 만든 다음 대상 배포용으로 내보내고 최적화할 가능성이 큽니다. 파이프라인은 다음과 같을 수 있습니다: YOLOv8n(커스텀) 학습 → ONNX로 내보내기 → PC에서 ONNX Runtime 실행, 그리고 별도로 Android용으로 TFLite(INT8)로 내보내기. 일반적인 Android 기기(중급형 2021년 휴대전화)에서 양자화된 YOLOv8n은 416×416 입력에서 약 20–30 FPS, EfficientDet-Lite0은 320×320에서 약 10–20 FPS로 동작할 것으로 예상되며, 이는 보고된 벤치마크(cytron.io, ejtech.io)를 기반으로 합니다. 이러한 성능은 투구 연습 시나리오에서 거의 실시간 피드백을 제공하기에 충분합니다.

## Integration with 3D Coordinate Estimation (ArUco & Pinhole Model)3D 좌표 추정(ArUco 및 핀홀 모델)과의 통합

A major component of the system is converting 2D detections into real-world 3D positions to judge strikes and balls. The existing solution establishes a **local 3D coordinate frame** using an ArUco marker on the ground as the origin. The camera is calibrated (rotation, translation, focal length) with respect to this marker, so we know the projection geometry. The prior method detected a colored ball and obtained its **pixel coordinates (u,v)** and **pixel radius**. Depth Z (distance from camera) was then computed by the pinhole camera formula:시스템의 주요 구성 요소는 2D 탐지를 실세계 3D 위치로 변환하여 스트라이크와 볼을 판정하는 것이다. 기존 솔루션은 지면에 놓인 ArUco 마커를 원점으로 하여 국소 3D 좌표계를 설정한다. 카메라는 이 마커에 대해 회전·병진·초점거리 등 보정이 되어 있으므로 투영 기하를 알고 있다. 이전 방법은 색상 공을 검출하여 그 픽셀 좌표 (u,v)와 픽셀 반지름을 얻었다. 그런 다음 깊이 Z(카메라로부터의 거리)는 핀홀 카메라 공식으로 계산되었다:

 

Z  =  f×Rrealrpixel,Z=Z=rrpixel픽셀​​ff××RRreal진짜​​​​,,

 

where _f_ is the camera focal length (in pixels), _R_real_ is the real radius of the baseball, and _r_pixel_ is the observed radius in the image. This formula comes directly from the similarity of triangles in projection (essentially, the ball’s apparent size inversely relates to distance). By using the known baseball diameter (~7.3 cm), the system could estimate how far the ball is from the camera at each frame in real time, as shown in their results (Figure 4 in the reference). Once Z was known, the 2D image center (u,v) could be back-projected into the 3D coordinate system. Specifically, they likely cast a ray from the camera through the pixel (using camera intrinsics and extrinsics) and found the 3D point along that ray at the distance Z. This yields (X, Y, Z) in marker coordinates, where Y might be the forward distance (towards home plate) and X the horizontal offset, Z the vertical height (depending on axis definitions).여기서 f는 카메라 초점거리(픽셀 단위), R_real은 야구공의 실제 반지름, r_pixel은 이미지에서 관측된 반지름이다. 이 공식은 투영에서의 삼각형 닮음에 직접적으로 기인한 것으로(본질적으로 공의 겉보기 크기는 거리와 반비례한다), 알려진 야구공 직경(약 7.3 cm)을 사용하면 시스템은 각 프레임에서 공이 카메라로부터 얼마나 떨어져 있는지 실시간으로 추정할 수 있었다(참고문헌의 그림 4에 제시된 결과 참조). Z를 알게 되면 2D 이미지 중심점(u,v)을 3D 좌표계로 역투영할 수 있다. 구체적으로, 그들은 카메라 내부·외부 파라미터를 사용해 카메라에서 해당 픽셀을 통과하는 광선을 쏘고 그 광선 상에서 거리 Z에 있는 3D 점을 찾았을 가능성이 높다. 이것은 마커 좌표계에서 (X, Y, Z)를 얻어주며, 축 정의에 따라 Y는 홈 플레이트 쪽으로의 전방 거리, X는 수평 편차, Z는 수직 높이가 될 수 있다.

 

To integrate the **deep learning detector**, we will follow a similar pipeline: for each frame where YOLO detects the ball, we obtain the **bounding box** (or segmentation) and infer the ball’s pixel center and size. YOLOv8n will give a bounding box width/height; for a roughly spherical object, we can take the average of width and height as the diameter in pixels. Alternatively, if the detector is very accurate, the box should tightly bound the ball, so half the box width ≈ r_pixel. (If the box is not perfectly tight due to blur, one might introduce a small scaling factor or use image processing to refine it – for example, apply a Hough circle detection _inside_ the YOLO bounding box to precisely locate the ball’s edges). With the estimated pixel radius, we apply the same pinhole equation to get Z. Because our camera is calibrated via ArUco, we know _f_ and have undistorted the image, so this depth calculation is straightforward and fast. One caveat: if the ball is extremely small (far away), the quantization of a few pixels can cause noise in Z; but within the ~18.44 m pitching distance, the ball will grow in the image as it approaches – our algorithm can compute Z on each frame where detection is confident.딥러닝 검출기를 통합하기 위해 우리는 유사한 파이프라인을 따릅니다: YOLO가 공을 검출한 각 프레임에서 바운딩 박스(또는 세그멘테이션)를 얻고 공의 픽셀 중심과 크기를 추정합니다. YOLOv8n은 바운딩 박스의 너비/높이를 제공하며, 대체로 구형 물체의 경우 너비와 높이의 평균을 픽셀 단위 지름으로 취할 수 있습니다. 또는 검출기가 매우 정확하다면 박스가 공을 꽉 잡아야 하므로 박스 너비의 절반 ≈ r_pixel로 볼 수 있습니다.(블러로 인해 박스가 완벽히 타이트하지 않은 경우 작은 스케일링 계수를 도입하거나 이미지 처리를 통해 보정할 수 있습니다 – 예를 들어 YOLO 바운딩 박스 내부에 허프 원 검출을 적용해 공의 가장자리를 정밀하게 찾는 방법이 있습니다). 추정된 픽셀 반지름을 가지고 동일한 핀홀 방정식을 적용해 Z를 구합니다. 카메라를 ArUco로 보정했고 이미지를 왜곡 보정했으므로 이 깊이 계산은 간단하고 빠릅니다. 한 가지 주의사항: 공이 매우 작게(멀리) 있을 경우 몇 픽셀의 양자화로 인해 Z에 노이즈가 발생할 수 있습니다; 그러나 약 18.44 m 투구 거리에서는 공이 접근함에 따라 이미지에서 커지므로 — 우리의 알고리즘은 검출이 신뢰할 수 있는 각 프레임에서 Z를 계산할 수 있습니다.

 

Once we have (X, Y, Z) in the real-world coordinates, determining a **strike** vs **ball** uses the predefined virtual strike zone. The system implements two parallel vertical planes (front and back of the strike zone) at the regulation distances (e.g. the front plane might align with the front edge of home plate and the back plane with the back edge). According to baseball rules (and KBO league’s ABS logic), a pitch is a strike if the ball passes through _the volume_ of the strike zone – meaning it must intersect both the front and back planes within the zone’s bounds. In practice, the algorithm does:실제 세계 좌표에서 (X, Y, Z)를 획득하면 스트라이크인지 볼인지는 미리 정의된 가상 스트라이크 존을 사용해 판정합니다. 시스템은 규정된 거리(예: 앞 평면은 홈 플레이트의 앞 가장자리와 정렬되고 뒷 평면은 뒷 가장자리와 정렬될 수 있음)에 따라 스트라이크 존의 앞뒤에 두 개의 병렬 수직 평면을 구현합니다. 야구 규칙(및 KBO 리그의 ABS 논리)에 따르면, 투구가 스트라이크인 것은 공이 스트라이크 존의 체적을 통과할 때로—즉, 존의 경계 내에서 앞·뒷 평면 둘 다와 교차해야 합니다. 실제로 알고리듬은 다음을 수행합니다:

1. Check that the 3D trajectory of the ball crosses the front plane (Z=some Y_front) and the back plane (Y_back). Given discrete frame observations, they likely monitor the ball’s Y coordinate: when it goes beyond the back plane, they know it has passed the zone. They also likely measure the shortest distance from the ball to each plane in real-time (which could be inferred by the sign change in position or a distance formula).공의 3D 궤적이 앞면 평면(Z=어떤 Y_front)과 뒷면 평면(Y_back)을 교차하는지 확인하세요. 이산 프레임 관측이 주어지면, 그들은 아마도 공의 Y 좌표를 모니터링합니다: 공이 뒷면 평면을 넘어가면 그 구역을 통과한 것을 압니다. 또한 실시간으로 공에서 각 평면까지의 최단 거리를 측정하는 경우가 많습니다(이는 위치의 부호 변화나 거리 공식을 통해 추정할 수 있습니다).
    
2. If the ball has passed both planes, then check the ball’s **horizontal position** (X and Z coordinates) at the moments it crossed those planes. They likely project the ball’s center onto each plane and see if it lies within the rectangle of the strike zone (which is defined by the width of home plate and the batter’s strike zone height). In the paper, they mention converting the ball’s 3D coords to the marker frame, then checking the distance to each plane and whether the ball’s image coordinates fall inside the strike zone area when the ball crossed the plane. Equation (2) in the paper formalizes the condition: essentially requiring the ball’s center to be between the top and bottom of the zone and between left and right bounds at the crossing moment. If both front-plane and back-plane crossings occurred within the zone boundaries, it’s a strike; if the ball went through the planes but was outside the zone rectangle at one of them, it’s a ball.만약 공이 두 평면을 모두 지나갔다면, 공이 그 평면들을 통과한 순간의 수평 위치(X 및 Z 좌표)를 검사한다. 보통 공의 중심을 각 평면에 투사하여 홈 플레이트의 너비와 타자의 스트라이크 존 높이로 정의된 스트라이크 존의 직사각형 안에 있는지 확인한다. 논문에서는 공의 3D 좌표를 마커 프레임으로 변환한 다음 각 평면까지의 거리를 확인하고 공이 평면을 통과했을 때 영상 좌표가 스트라이크 존 영역 안에 들어오는지를 점검한다고 언급한다. 논문의 식 (2)는 이 조건을 형식화한 것으로, 기본적으로 공의 중심이 통과 순간에 존의 상단과 하단 사이에 있고 좌우 경계 사이에 있어야 함을 요구한다. 전방 평면과 후방 평면의 통과가 모두 존 경계 내에서 발생하면 스트라이크이고, 공이 평면들을 통과했지만 그 중 하나에서 존 직사각형 밖에 있었다면 볼이다.
    

The deep learning model will supply more robust (u,v) positions even when the background is not uniform, which should improve the accuracy of the 3D trajectory. We will integrate the detection and tracking as follows: run YOLO on each frame → if detection confidence is high, compute the 3D position as above → feed the 3D position into the existing trajectory logic. We may use a simple Kalman filter or polynomial fit on the sequence of 3D points to smooth the trajectory, similar to how Justin Chou fitted a cubic polynomial through YOLO-detected centers to model a baseball’s flight[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=calculated%20the%20time%20the%20pitch,take%20to%20reach%20the%20plate)[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=To%20eliminate%20outliers%20within%20the,best%20results%20in%20my%20tests). Smoothing helps handle the fact that YOLO might occasionally miss the ball in a frame or two. Since we only have one camera, we assume a ballistic trajectory and could even fit a parabola in the vertical direction (including gravity effect) to predict the ball’s path through the strike zone. This can fill in any dropped detections and give us a continuous path. The final strike/ball decision logic remains the same, just using the smoothed 3D path. In essence, the deep model replaces the color-threshold + blob detection step, feeding into the pinhole depth estimation (which is kept the same formula) and the AR strike zone intersection test.딥러닝 모델은 배경이 균일하지 않은 경우에도 더 견고한 (u,v) 위치를 제공하여 3D 궤적의 정확도를 향상시킬 것입니다. 탐지와 추적을 다음과 같이 통합할 것입니다: 각 프레임에 대해 YOLO를 실행 → 탐지 신뢰도가 높으면 위와 같이 3D 위치를 계산 → 계산한 3D 위치를 기존의 궤적 로직에 입력. 우리는 간단한 칼만 필터나 3D 점들의 시퀀스에 대한 다항식 적합을 사용해 궤적을 평활화할 수 있는데, 이는 Justin Chou가 YOLO로 탐지한 중심점을 통해 3차 다항식을 적합해 야구의 비행을 모델링한 방식과 유사합니다 medium.commedium.com.평활화는 YOLO가 가끔 한두 프레임에서 공을 놓칠 수 있다는 사실을 처리하는 데 도움이 됩니다. 카메라가 하나뿐이므로 탄도 궤적을 가정하고 수직 방향(중력 효과 포함)에 포물선을 적합하여 스트라이크 존을 통과하는 공의 경로를 예측할 수도 있습니다. 이는 누락된 탐지를 보완하고 연속적인 경로를 제공할 수 있습니다. 최종 스트라이크/볼 판정 로직은 동일하게 유지되며, 다만 평활화된 3D 경로를 사용합니다.본질적으로 딥 모델은 색상 임계치 + 블롭 탐지 단계를 대체하며, 고정된 공식의 핀홀 깊이 추정으로 전달되고 AR 스트라이크 존 교차 테스트로 이어집니다.

 

An additional consideration is **synchronization and latency**: The detection model will introduce a small delay per frame. If running on the phone, it might process ~30 FPS with ~33 ms latency; on a PC GPU, much faster. But even ~30 ms could slightly offset the frame timestamp. However, since we are mostly concerned with where the ball is when crossing the zone, a minor delay is not critical, and the processing is still sequentially in real-time. The visualization can be done by overlaying the virtual strike zone and highlighting the ball’s position as before – the AR overlay code (drawing the zone, the ball trace, and outcomes) can remain as implemented.추가로 고려할 점은 동기화와 지연 시간입니다: 검출 모델은 프레임당 약간의 지연을 유발합니다. 휴대폰에서 실행하면 약 30 FPS에 약 33ms의 지연이 발생할 수 있고, PC GPU에서는 훨씬 더 빠릅니다. 하지만 약 30ms 정도는 프레임 타임스탬프를 약간 어긋나게 할 수 있습니다. 다만 우리가 주로 관심 있는 것은 공이 존을 지날 때의 위치이므로, 소소한 지연은 치명적이지 않으며 처리도 실시간으로 순차적으로 이루어집니다. 시각화는 이전과 같이 가상 스트라이크 존을 오버레이하고 공의 위치를 강조하여 수행할 수 있습니다 — AR 오버레이 코드(존 그리기, 공의 궤적, 판정 표시)는 현재 구현된 대로 유지할 수 있습니다.

 

Overall, combining the learned detector with the existing geometric pipeline leverages the best of both: robust detection in difficult visuals, plus precise physical localization using known camera calibration. This methodology (detect → find 3D ray via ArUco calibration → use known object size for depth) is efficient and was proven in the current system with color tracking. We simply expect the detection rate to improve dramatically with the deep model, allowing consistent _3D tracking_ of the pitched ball until it crosses the plate.전반적으로, 학습된 검출기를 기존의 기하학적 파이프라인과 결합하면 둘의 장점을 모두 활용할 수 있습니다: 어려운 시각 조건에서도 강인한 검출 성능과, 알려진 카메라 보정값을 이용한 정밀한 물리적 위치 추정이 그것입니다. 이 방법론(검출 → ArUco 보정으로 3D 광선 찾기 → 알려진 물체 크기로 깊이 계산)은 효율적이며, 현재 시스템에서 색상 추적으로 이미 검증되었습니다. 우리는 단지 딥 모델을 사용함으로써 검출률이 극적으로 향상되어 공이 홈 플레이트를 통과할 때까지 일관된 3D 추적이 가능해질 것으로 기대합니다.

## Deployment Architecture: On-Device vs Offloaded Inference배포 아키텍처: 장치 내(온디바이스) 대 오프로딩된 추론

We compare two system architectures for running the model:우리는 모델 실행을 위한 두 가지 시스템 아키텍처를 비교합니다:

1. **Mobile-Only Inference (On-Device):** In this setup, the smartphone (or a low-power PC) both captures video and runs the detection model locally. The advantages are low latency (no network transmission of frames) and simplicity – everything is self-contained. Modern phones with NPUs/GPU acceleration can handle a model like YOLOv8n or EfficientDet-Lite in real-time. For instance, a recent iPhone or high-end Android can likely achieve 30+ FPS with a quantized tiny model. On mid-range hardware, we might get 10–20 FPS which could still be acceptable for feedback (especially if we only need to process ~60 frames during the pitch trajectory of ~0.5 seconds). Running on-device also ensures portability – you could bring the system to a field with just a phone and marker, no laptop needed. Power consumption is a factor: continuous model inference will drain battery and heat up the device, but since pitching practice is not continuous video for hours, this might be manageable (short bursts of activity).모바일 전용 추론(디바이스 내): 이 구성에서는 스마트폰(또는 저전력 PC)이 영상 캡처와 탐지 모델 실행을 모두 로컬에서 수행합니다. 장점은 지연 시간이 낮고(프레임을 네트워크로 전송할 필요 없음) 단순하다는 점이며, 모든 것이 자체적으로 처리된다는 것입니다. NPU/GPU 가속을 갖춘 최신 폰은 YOLOv8n이나 EfficientDet-Lite 같은 모델을 실시간으로 처리할 수 있습니다. 예를 들어 최근형 아이폰이나 고급형 안드로이드는 양자화된 소형 모델로 30FPS 이상을 달성할 가능성이 큽니다. 중급 하드웨어에서는 10–20FPS를 얻을 수 있는데, 이는 피드백 용도로(특히 약 0.5초 정도의 피치 궤적 동안 약 60프레임만 처리하면 되는 경우) 여전히 수용할 만할 수 있습니다. 디바이스 내 실행은 휴대성도 보장하므로, 노트북 없이 폰과 마커만으로 현장에 시스템을 가져갈 수 있습니다. 전력 소비가 고려 사항인데, 지속적인 모델 추론은 배터리를 소모하고 기기를 발열시킵니다. 다만 피칭 연습은 수시간 계속되는 연속 영상이 아니기 때문에(짧은 활동의 반복) 관리 가능할 수 있습니다.
    
     
    
    _Feasibility:_ Given that our baseline system already ran in a **“CPU-only environment” in real-time with color tracking*, replacing that with a neural model is feasible if optimized. Quantized inference on mobile CPUs can reach near-real-time speeds (tens of milliseconds per frame) for a tiny model[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Inference%20Speed%20). Additionally, if the phone has a GPU or NNAPI support, we can use that to speed up inference. One challenge is integrating ArUco marker tracking on the phone – we would need to use OpenCV (which exists for Android) to detect the marker and get the calibration each session. That computation is lightweight (it only runs once or at low framerate to adjust the coordinate frame). The rest (drawing AR, etc.) can be done with OpenGL or ARCore if we get fancy, or simply by drawing on frames. Given the phone’s camera will be used, we have to ensure the camera feed can be provided to both the ArUco detector and the neural model. This might require camera2 API with a custom pipeline, or using an existing AR library. But since it currently works on PC, an Android port using OpenCV’s Java/C++ interface is quite possible.실현 가능성: 기본 시스템이 이미 색상 추적으로 실시간 “CPU 전용 환경”에서 실행되었으므로, 최적화한다면 이를 신경망 모델로 교체하는 것은 실현 가능하다. 모바일 CPU에서 양자화된 추론은 작은 모델의 경우 프레임당 수십 밀리초 수준의 준실시간 속도에 도달할 수 있다(cytron.io). 또한 휴대폰에 GPU나 NNAPI 지원이 있다면 이를 사용해 추론 속도를 높일 수 있다. 한 가지 과제는 휴대폰에서 ArUco 마커 추적을 통합하는 것인데, 매 세션마다 마커를 감지하고 보정값을 얻기 위해 OpenCV(안드로이드를 위한 구현이 있음)를 사용해야 한다. 그 연산은 가볍다(좌표계를 조정하기 위해 한 번만 실행되거나 낮은 프레임레이트로만 실행됨). 나머지(AR 렌더링 등)는 OpenGL이나 ARCore를 사용해 세련되게 처리할 수 있고, 단순히 프레임에 그리는 방식으로도 가능하다. 휴대폰의 카메라가 사용될 것이므로 카메라 피드를 ArUco 검출기와 신경망 모델 모두에 제공할 수 있도록 해야 한다. 이를 위해서는 커스텀 파이프라인을 갖춘 camera2 API 사용이나 기존 AR 라이브러리 사용이 필요할 수 있다. 그러나 현재 PC에서 작동하므로 OpenCV의 Java/C++ 인터페이스를 이용한 안드로이드 포팅은 충분히 가능하다.
    
2. **Phone Camera + Laptop Inference (Offloaded):** Here the phone (or an IP camera) streams the video feed to a laptop, which runs the detection model and AR visualization. The benefit is we can utilize a more powerful processor/GPU on the laptop, possibly running a larger model or achieving higher FPS with ease. This could also simplify development by leveraging existing PC code (since the original system was PC-based). The downside is the streaming latency and network setup. If using Wi-Fi to stream video, we introduce maybe 50-100 ms latency and potential frame drops. If the phone is tethered via USB (e.g. using it as a webcam via USB), latency can be quite low (~<50 ms) and bandwidth is high. There are existing apps that turn a smartphone into a webcam (DroidCam, etc.), or we could use RTSP streaming. The laptop can then process frames as they arrive.휴대폰 카메라 + 노트북 추론(오프로드): 여기서 휴대폰(또는 IP 카메라)은 비디오 피드를 노트북으로 스트리밍하고, 노트북이 검출 모델 및 AR 시각화를 실행합니다. 장점은 노트북의 더 강력한 프로세서/GPU를 활용할 수 있어 더 큰 모델을 돌리거나 더 높은 FPS를 쉽게 달성할 수 있다는 점입니다. 또한 원래 시스템이 PC 기반이었기 때문에 기존 PC 코드를 활용해 개발을 단순화할 수도 있습니다. 단점은 스트리밍 지연(latency)과 네트워크 설정입니다. 비디오를 Wi-Fi로 스트리밍하면 대략 50–100ms의 지연과 잠재적인 프레임 드롭이 발생할 수 있습니다. 휴대폰을 USB로 테더링(예: USB를 통해 웹캠으로 사용)하면 지연이 매우 낮을 수(~<50ms) 있고 대역폭도 높습니다. 스마트폰을 웹캠으로 바꿔주는 기존 앱(DroidCam 등)을 사용하거나 RTSP 스트리밍을 사용할 수 있습니다. 노트북은 도착하는 프레임을 처리하면 됩니다.
    
     
    
    With offloading, we could even run a heavier model like YOLOv8s or a higher input resolution because of the laptop’s greater compute, getting more accuracy. However, for the purpose of a low-cost system, needing a laptop diminishes portability. It might be acceptable for a training session in a fixed location (e.g. a coach with a laptop by the field), but one goal was to keep the equipment minimal. Another consideration: synchronization between camera frames and processing. If the network delay is variable, our tracking might suffer (but likely minor since baseball flight is short and we process sequentially).오프로딩을 사용하면 노트북의 더 큰 연산 성능 덕분에 YOLOv8s 같은 더 무거운 모델이나 더 높은 입력 해상도를 실행할 수 있어 정확도를 높일 수 있다. 하지만 저비용 시스템을 목표로 할 경우 노트북이 필요하다는 점은 휴대성을 떨어뜨린다. 이는 고정된 장소에서의 훈련 세션(예: 경기장 옆에 노트북을 둔 코치)에는 허용될 수 있지만 장비를 최소화하려는 목표와는 맞지 않는다. 또 다른 고려사항은 카메라 프레임과 처리 간의 동기화이다. 네트워크 지연이 가변적이면 우리의 추적에 지장이 있을 수 있다(하지만 야구공의 비행이 짧고 순차적으로 처리하기 때문에 아마도 미미할 것이다).
    
     
    
    _Feasibility:_ The original implementation already ran on a PC (with OpenCV). We can replace the color detection with our model running via ONNX Runtime or PyTorch on that PC. If the PC has a modest GPU (even an NVIDIA GTX 1650 or a laptop iGPU), YOLOv8n can run at hundreds of FPS[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=FPS)[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=YOLOv8n), so processing 60 frames of a pitch is trivial. The critical part is capturing frames from the phone in high quality; a wired connection is preferred for reliability. One could also mount a USB webcam directly and avoid using the phone camera at all in this architecture.실현 가능성: 원래 구현은 이미 PC에서(OpenCV로) 실행되었습니다. 색상 검출을 모델이 ONNX Runtime이나 PyTorch로 해당 PC에서 실행되도록 교체할 수 있습니다. PC에 적당한 GPU(심지어 NVIDIA GTX 1650이나 랩탑 iGPU)가 있다면 YOLOv8n은 수백 FPS로 실행될 수 있으므로7universum.com7universum.com 피치의 60프레임 처리도 사소한 일입니다. 핵심은 휴대전화에서 고화질로 프레임을 캡처하는 것이며, 신뢰성을 위해 유선 연결이 선호됩니다. 이 아키텍처에서는 USB 웹캠을 직접 장착해 휴대전화 카메라를 전혀 사용하지 않는 방법도 가능합니다.
    

**Comparison:** On-device (phone-only) is more _accessible and portable_ – a coach or player could use just their phone on a tripod at the bullpen. Offloaded (phone + PC) is more _powerful_ – allowing potentially better accuracy and easier development with powerful hardware. Latency-wise, a phone running at 15–20 FPS might actually detect the ball in fewer frames than a 60 FPS PC stream (since the mobile might skip some frames due to lower FPS). However, that may not be an issue if those frames are enough. A PC at high FPS could give very smooth tracking. If latency (from camera capture to result) stays under ~0.1–0.2 s for either solution, the _perceived real-time_ feedback would be fine (for example, an umpire call a tenth of a second late is not noticeable).비교: 기기 내 처리(폰 전용)는 더 접근하기 쉽고 휴대성이 좋다 — 코치나 선수는 불펜에 삼각대에 올린 폰만으로 사용할 수 있다. 오프로딩(폰 + PC)은 더 강력하다 — 강력한 하드웨어로 더 나은 정확도와 쉬운 개발이 가능하다. 지연 측면에서 보면, 15–20 FPS로 동작하는 폰은(프레임이 적어 일부가 건너뛰더라도) 60 FPS의 PC 스트림보다 실제로 공을 더 적은 프레임에서 감지할 수 있다. 다만 그 프레임들이 충분하다면 문제가 되지 않을 수 있다. 고FPS의 PC는 매우 부드러운 추적을 제공할 수 있다. 카메라 캡처부터 결과까지의 지연이 어느 쪽이든 대략 0.1–0.2초 이내라면, 현실적으로 인지되는 실시간 피드백은 괜찮을 것이다(예: 심판의 판정이 0.1초 늦어져도 눈에 띄지 않는다).

 

One scenario is to implement both: e.g., use the phone alone for ease, but have an option where the phone sends video to a server (PC or cloud) for processing, which could be useful if one wanted to gather data from multiple cameras or do heavier analysis. For now, given the question context, the primary focus is likely to ensure even a **low-spec PC or Android** can run this – which our exploration shows is possible with a quantized lightweight model[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=YOLOv8n%20gave%20the%20best%20results,speed%20and%20accuracy%20are%20important)[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Inference%20Speed%20).한 가지 시나리오는 둘 다 구현하는 것입니다: 예를 들어 사용 편의성을 위해 휴대전화만 사용하되, 휴대전화에서 비디오를 서버(PC 또는 클라우드)로 전송하여 처리할 수 있는 옵션을 두는 것입니다. 이는 여러 대의 카메라에서 데이터를 수집하거나 더 무거운 분석을 수행하려는 경우 유용할 수 있습니다. 현재 질문의 맥락을 고려할 때, 주된 초점은 저사양 PC나 Android에서도 실행할 수 있도록 하는 것으로 보이며 — 우리의 탐색 결과에 따르면 양자화된 경량 모델로 가능함을 보여줍니다7universum.comcytron.io.

## Academic Contribution and Evaluation (Novelty, Utility, Reproducibility)학문적 기여 및 평가(신규성, 유용성, 재현성)

From a research perspective, we consider how this system might be viewed by reviewers at top venues like CVPR, ICCV (vision conferences) or CHI/UIST (HCI and interactive systems):연구 관점에서, 우리는 이 시스템이 CVPR, ICCV(비전 학회) 또는 CHI/UIST(HCI 및 인터랙티브 시스템)와 같은 최고 수준의 학회 심사자들에게 어떻게 평가될지에 대해 고려한다:

- **Novelty:** The components of the system leverage known techniques – lightweight object detectors (YOLO, SSD) and standard AR/vision algorithms (marker-based calibration, pinhole geometry). On its face, this is an _engineering integration_ project rather than a fundamentally new algorithm. CVPR/ICCV reviewers typically look for methodological innovations or new state-of-the-art results. Simply applying YOLO and ARUco to build a strike zone system might be seen as incremental. However, there may be novelty in the specific _domain application_: a **single-camera, low-cost automated strike zone** is not widely seen in literature. Professional systems (e.g. MLB’s Statcast or Hawk-Eye) use expensive multi-camera or radar setups; showing that a cheap monocular system can approximate that function could be considered an innovation in sports technology. The key would be demonstrating _how well it works_ (accuracy of calls) and what new techniques were needed to overcome challenges (e.g. specialized data augmentations for fast objects, or combining detection with physics modeling). If those aspects are emphasized, a specialized venue (like a sports analytics workshop or an augmented reality conference) might appreciate the contribution. For a main CVPR paper, the work would need a stronger technical contribution – perhaps a new small-object detection method or a novel real-time tracking algorithm. Without such a novel algorithm, CVPR/ICCV might deem it an application paper with limited novelty. **UIST/CHI** could find it interesting if framed as an interactive training tool – especially if there is an evaluation with users (players or coaches) demonstrating improved training outcomes or a novel AR interface for feedback. The AR aspect (visualizing a virtual strike zone and pitch locations in real time) and the fact that it’s enabling a new experience for amateur players could appeal to HCI reviewers, provided the system is implemented and evaluated in practice.신규성: 시스템 구성 요소들은 이미 알려진 기법들—경량 물체 탐지기(YOLO, SSD)와 표준 AR/비전 알고리즘(마커 기반 보정, 핀홀 기하학)—을 활용한다. 겉으로 보기에는 근본적으로 새로운 알고리즘이라기보다 공학적 통합 프로젝트에 가깝다. CVPR/ICCV 심사자들은 일반적으로 방법론적 혁신이나 새로운 최첨단 결과를 기대한다. 단순히 YOLO와 ARUco를 적용해 스트라이크 존 시스템을 구축하는 것은 점진적 접근으로 보일 수 있다. 다만 특정 도메인 적용에서 새로움이 있을 수 있다: 단일 카메라 기반의 저비용 자동 스트라이크 존은 문헌에서 널리 다뤄지지 않았다. 상업적 시스템(예: MLB의 Statcast나 Hawk-Eye)은 고가의 다중 카메라 또는 레이더 구성을 사용한다; 저렴한 단안 시스템이 그 기능을 근사할 수 있음을 보여준다면 스포츠 기술 분야에서 혁신으로 간주될 수 있다. 핵심은 얼마나 잘 작동하는지를(판정 정확도) 입증하고, 문제를 극복하기 위해 어떤 새로운 기법들이 필요했는지(예: 빠른 물체를 위한 특수 데이터 증강, 또는 탐지와 물리 모델링의 결합)를 보여주는 것이다.이런 측면들이 강조된다면, 스포츠 분석 워크숍이나 증강현실 학회와 같은 전문 행사에서는 기여를 높게 평가할 수 있다. 메인 CVPR 논문으로는 더 강력한 기술적 기여가 필요할 것이다—예를 들어 새로운 소형 물체 탐지 기법이나 실시간 추적 알고리즘의 제안 등. 그러한 새로운 알고리즘이 없다면 CVPR/ICCV는 이를 참신성이 제한된 응용 사례 논문으로 판단할 수 있다. UIST/CHI는 상호작용형 훈련 도구로서 구성하면 흥미롭게 여길 수 있는데—특히 사용자(선수나 코치)를 대상으로 한 평가를 통해 훈련 성과 개선을 입증했거나 피드백을 위한 참신한 AR 인터페이스를 제시했다면 더욱 그렇다. AR 측면(가상 스트라이크 존과 투구 위치를 실시간으로 시각화하는 것)과 아마추어 선수들에게 새로운 경험을 제공한다는 점은 시스템이 실제로 구현되고 평가되었다면 HCI 심사자들에게 어필할 수 있다.
    
- **Utility:** There is clear practical utility, as noted in the original paper: amateurs and youth players seldom have access to expensive Automated Ball-Strike (ABS) systems. A low-cost solution that only needs a camera and a marker could democratize this technology as a training aid. Reviewers who value real-world impact (e.g. CHI audiences, or applied science committees) would recognize this system’s usefulness. Utility can also be highlighted by comparing with existing solutions: e.g., radar-based systems are costly; multi-camera systems need complex calibration. Here we show that with smart use of computer vision, one camera can deliver a comparable function. The system’s usefulness can be quantified by accuracy (how often does it call the pitch correctly compared to an official system or an umpire) and by user feedback (does it help batters practice discipline? does it help pitchers locate their throws?). If included in a paper, a field experiment demonstrating those would strengthen the utility claim.유용성: 원 논문에서 언급한 바와 같이 명확한 실용적 유용성이 있다. 아마추어나 청소년 선수들은 고가의 자동 볼-스트라이크(ABS) 시스템을 거의 이용할 수 없다. 카메라와 마커만 필요로 하는 저비용 솔루션은 훈련 보조기구로서 이 기술을 민주화할 수 있다. 실제 영향력을 중시하는 리뷰어들(예: CHI 청중이나 응용과학 심사위원회)은 이 시스템의 유용성을 인식할 것이다. 유용성은 기존 솔루션과의 비교를 통해서도 강조될 수 있다. 예컨대 레이더 기반 시스템은 비용이 많이 들고, 다중 카메라 시스템은 복잡한 보정이 필요하다. 여기서는 컴퓨터 비전의 영리한 사용을 통해 하나의 카메라로도 유사한 기능을 제공할 수 있음을 보인다. 시스템의 유용성은 정확도(공식 시스템이나 심판과 비교해 얼마나 자주 투구 판정을 올바르게 내리는가)와 사용자 피드백(타자가 규율 훈련에 도움이 되는가? 투수가 투구 위치를 찾는 데 도움이 되는가?)으로 정량화할 수 있다. 논문에 포함된다면, 이를 입증하는 현장 실험은 유용성 주장을 강화할 것이다.
    
- **Reproducibility:** From a reproducibility standpoint, the project fares well. It uses **open-source libraries (OpenCV, ArUco) and standard model architectures**, and we have identified open datasets for training. By publishing the model weights and dataset (which the team is likely to do, given they reference open-source aims[github.com](https://github.com/BaseballCV/BaseballCV#:~:text=Our%20goal%20is%20to%20provide,tools%20that%20analyze%20baseball%20games)), others can replicate the results. The hardware requirements are minimal – any modern laptop or smartphone can run the system – which lowers the barrier to reproduction. If writing an academic article, providing the exact calibration method, the model hyperparameters, and perhaps a link to a code repository would satisfy reproducibility criteria. Top conferences also appreciate when authors release code and data; doing so would be a plus. In terms of evaluation reproducibility, one could imagine instructions like “set up an ArUco marker of this size at home plate, position a camera 18m away at X height, run our program” and one should observe the AR overlay and strike/ball calls as described. This straightforward setup is actually a strength – it’s not reliant on any proprietary sensor.재현성: 재현성 관점에서 이 프로젝트는 성과가 좋은 편입니다. 오픈소스 라이브러리(OpenCV, ArUco)와 표준 모델 아키텍처를 사용하고 있으며, 학습을 위한 공개 데이터셋도 확인했습니다. 모델 가중치와 데이터셋을 공개하면(팀이 공개 지향성(aimsgithub.com)을 언급한 점으로 보아 그럴 가능성이 높음) 다른 이들이 결과를 재현할 수 있습니다. 하드웨어 요구사항도 최소한이라 현대적인 노트북이나 스마트폰이면 시스템을 실행할 수 있어 재현 장벽을 낮춥니다. 학술 논문을 작성할 경우 정확한 캘리브레이션 방법, 모델 하이퍼파라미터, 그리고 코드 저장소 링크를 제공하면 재현성 기준을 충족할 것입니다. 주요 학회들도 저자들이 코드와 데이터를 공개하는 것을 높이 평가하며, 그렇게 하면 장점이 됩니다. 평가 재현성 측면에서는 “이 크기의 ArUco 마커를 홈 플레이트에 설치하고, 카메라를 X 높이에서 18m 떨어진 위치에 두고 프로그램을 실행하라”와 같은 지침을 상상할 수 있으며, 그러면 기술된 대로 AR 오버레이와 스트라이크/볼 판정이 관찰되어야 합니다. 이 간단한 설치는 실제로 강점으로 작용합니다 — 어떤 독점 센서에 의존하지 않습니다.
    

In the context of a CVPR/ICCV _review_: They might say the system is well-engineered but ask “where is the new science?” To address that, one could underscore the **technical challenges tackled**: for example, achieving high precision detection of a ~20×20 pixel object moving at 40 m/s under varying lighting _is_ non-trivial, and our solution combining data augmentation and model optimization is what makes it work. It’s an opportunity to claim a bit of novelty in the **combination of techniques** for a unique problem. Additionally, integrating the AR visualization with computer vision in real-time could interest conferences like **ISMAR (AR) or smaller CV apps venues**. CHI/UIST reviewers would ask about the **user experience**: Is the system easy to use? Does it provide feedback in a useful way (e.g., a visual overlay, or voice saying “Strike!”)? Are there novel interaction elements (perhaps a mobile app interface to review one’s pitching stats, etc.)? We might not have focused on that yet, but adding a simple UI (like a mobile scoreboard that shows counts of strikes/balls, and highlights the last pitch location) was mentioned and would enhance the contribution from a user perspective.CVPR/ICCV 리뷰 맥락에서: 그들은 시스템이 잘 설계되었다고 말할 수 있지만 “새로운 과학이 어디 있나?”라고 물을 수 있다. 이에 대응하기 위해 다룬 기술적 난관을 강조할 수 있다: 예를 들어, 조명이 변하는 상황에서 초당 약 40 m/s로 이동하는 대략 20×20 픽셀 크기의 물체를 고정밀로 검출하는 것은 간단하지 않으며, 데이터 증강과 모델 최적화를 결합한 우리의 해결책이 이를 가능하게 한다는 점이다. 이는 독특한 문제에 대한 기법들의 조합에서 어느 정도의 새로움을 주장할 기회이다. 또한 AR 시각화를 컴퓨터 비전과 실시간으로 통합한 점은 ISMAR(AR)이나 소규모 CV 응용 학회들의 관심을 끌 수 있다. CHI/UIST 리뷰어들은 사용자 경험에 대해 물을 것이다: 시스템은 사용하기 쉬운가? 유용한 방식으로 피드백을 제공하는가(예: 시각적 오버레이나 “스트라이크!”라는 음성 안내 등)? 새로운 상호작용 요소가 있는가(예: 자신의 투구 기록을 확인할 수 있는 모바일 앱 인터페이스 등)? 우리는 아직 그 부분에 집중하지 않았을 수도 있지만, 간단한 UI(스트라이크/볼 수를 보여주고 마지막 투구 위치를 강조하는 모바일 점수판 같은)를 추가하는 것은 언급되었고 사용자 관점에서 기여를 향상시킬 것이다.

 

**Contribution Value:** Overall, this work can be a valuable **applied research contribution** demonstrating how far inexpensive computer vision can go in sports analytics. It likely would not be accepted to a Tier-1 vision conference as a _full paper_ without additional CV algorithm contributions. However, it could be a **workshop paper or demo** in a vision conference (e.g. CVPR Workshops on Computer Vision in Sports) – especially if coupled with a thorough evaluation. The authors might also target journals or conferences in sports science or pervasive computing. If academic publication is a goal, emphasizing the **system’s accuracy and innovation in deployment** (e.g. “first system to do 3D strike zone tracking with a single $100 camera in real time”) and comparing against other methods (like how much error compared to TrackMan or to human ump calls) will be important. The novelty in the _HCI sense_ could be in enabling new training routines – that might interest CHI if a user study is conducted.기여 가치: 전반적으로 이 연구는 저비용 컴퓨터 비전이 스포츠 분석 분야에서 얼마나 활용될 수 있는지를 보여주는 가치 있는 응용 연구 기여가 될 수 있습니다. 추가적인 CV 알고리즘 기여 없이 전체 논문으로는 Tier-1 비전 학회에서 채택되기 어려울 가능성이 큽니다. 그러나 철저한 평가와 함께라면 비전 학회의 워크숍 논문이나 데모(예: CVPR의 Computer Vision in Sports 워크숍)로는 적합할 수 있습니다. 저자들은 스포츠 과학이나 퍼베이시브 컴퓨팅 분야의 저널이나 학회를 목표로 삼을 수도 있습니다. 학술 출판이 목표라면 시스템의 정확성과 배포상의 혁신(예: “단일 $100짜리 카메라로 실시간 3D 스트라이크 존 추적을 수행한 최초의 시스템”)을 강조하고 다른 방법들과의 비교(예: TrackMan이나 심판의 판정과 비교했을 때 오차가 얼마나 되는지)를 제시하는 것이 중요할 것입니다. HCI 관점에서의 참신성은 새로운 훈련 루틴을 가능하게 하는 데 있을 수 있으며, 사용자 연구가 수행된다면 CHI의 관심을 끌 수 있습니다.

 

Finally, from a **reproducibility and openness** standpoint: the fact that this system is built on open-source tools and potentially provides an open dataset (like BaseballCV’s) aligns well with community values[github.com](https://github.com/BaseballCV/BaseballCV#:~:text=Our%20goal%20is%20to%20provide,tools%20that%20analyze%20baseball%20games). An academic reviewer would commend that, as it means others can build on this work easily. Ensuring the approach is well documented (maybe releasing the labeled dataset of green-ball pitches used in development, etc.) will strengthen that aspect.마지막으로, 재현성 및 개방성 관점에서: 이 시스템이 오픈 소스 도구를 기반으로 구축되었고 잠재적으로 BaseballCV와 같은 오픈 데이터셋(예: github.com)을 제공한다는 사실은 커뮤니티의 가치와 잘 맞습니다. 학술 심사자는 다른 사람들이 이 작업을 쉽게 확장할 수 있다는 점에서 이를 칭찬할 것입니다. 접근법을 잘 문서화하고(개발에 사용된 녹색 공(그린볼) 투구의 라벨이 붙은 데이터셋을 공개하는 등) 공개하면 이 측면이 강화될 것입니다.

## Conclusion결론

In conclusion, introducing a lightweight deep learning detector into the AR strike zone system appears both feasible and beneficial. Models like YOLOv8n offer significantly improved ball detection accuracy and contextual robustness, at the cost of some processing overhead that is manageable with optimizations (quantization, TensorRT/ONNX acceleration). Proper training on augmented data (high-res, blurred and varied images of baseballs) will address the challenges of small, fast target detection. By combining the model’s 2D outputs with the established ArUco marker calibration and pinhole depth computation, we retain the ability to compute 3D trajectories and strike zone intersection precisely. The system can be deployed either as a standalone mobile app or with a split architecture using a PC, depending on resources – each with trade-offs in latency and convenience.결론적으로, AR 스트라이크 존 시스템에 경량 딥러닝 탐지기를 도입하는 것은 실현 가능하고 유익해 보입니다. YOLOv8n과 같은 모델은 일정한 처리 오버헤드가 발생하긴 하지만(양자화, TensorRT/ONNX 가속 등으로 관리 가능) 공의 탐지 정확도와 상황적 강인성을 크게 향상시킵니다. 고해상도, 블러 처리 및 다양한 야구공 이미지로 증강한 적절한 학습은 작고 빠르게 움직이는 표적 탐지의 문제를 해결할 것입니다. 모델의 2D 출력과 기존의 ArUco 마커 보정 및 핀홀 깊이 계산을 결합하면 3D 궤적과 스트라이크 존 교차를 정밀하게 계산하는 기능을 유지할 수 있습니다. 시스템은 자원에 따라 독립형 모바일 앱으로 배포하거나 PC를 사용하는 분산 아키텍처로 구성할 수 있으며, 각 방식은 지연 시간과 편의성 면에서 트레이드오프가 있습니다.

 

This project stands to make an **impact in accessible sports technology**, turning a complex umpiring aid into something any amateur league or coach could use with minimal setup. Academically, while it repurposes known techniques, it exemplifies a strong **applied innovation**: achieving a task with inexpensive means that traditionally required high-end setups. With thorough evaluation (both technical accuracy and user acceptance), it could be developed into a publishable case study in sports computer vision or AR interaction. The **novelty** largely lies in inventive integration and deployment, the **utility** is evident for training and education, and the **reproducibility** is high given the use of open models and data. In summary, adopting a lightweight deep learning approach is a promising next step that should substantially enhance the AR strike zone system’s performance and robustness, bringing it closer to a real-world usable tool.이 프로젝트는 복잡한 심판 보조 도구를 최소한의 설치로 어떤 아마추어 리그나 코치도 사용할 수 있는 접근 가능한 스포츠 기술로 바꾸어 큰 영향을 미칠 수 있습니다. 학문적으로는 알려진 기법들을 재활용하고 있지만, 전통적으로 고가의 장비가 필요했던 작업을 저비용 수단으로 달성했다는 점에서 강한 응용적 혁신의 사례를 보여줍니다. 철저한 평가(기술적 정확성 및 사용자 수용성 모두)를 거치면 스포츠 컴퓨터 비전 또는 AR 상호작용 분야의 출판 가능한 사례 연구로 발전시킬 수 있습니다. 참신성은 주로 창의적인 통합과 배치에 있고, 유용성은 훈련 및 교육에서 명백하며, 오픈 모델과 데이터의 사용으로 재현성도 높습니다. 요약하자면, 경량 딥러닝 접근법을 도입하는 것은 AR 스트라이크 존 시스템의 성능과 견고성을 상당히 향상시켜 실제 사용 가능한 도구에 더 가깝게 만드는 유망한 다음 단계입니다.

 

**Sources:** The analysis draws upon performance studies of YOLOv8n vs other models[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=As%20we%20can%20see%2C%20YOLOv8n,detections%2C%20like%20confusing%20trees%20with)[7universum.com](https://7universum.com/ru/tech/archive/item/20045#:~:text=MobileNet,unless%20retrained%20with%20better%20data), practical accounts of sports ball tracking with YOLO[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=For%20those%20unfamiliar%20with%20machine,identify%20objects%20within%20a%20frame)[medium.com](https://medium.com/@justintchou/baseball-pitch-overlays-with-yolov8-and-opencv-e80598c21e6b#:~:text=The%20bounding%20boxes%2C%20while%20useful%2C,and%20catch%20these%20edge%20cases), data augmentation best-practices for small objects[pdfs.semanticscholar.org](https://pdfs.semanticscholar.org/6926/10f36e339a631ab2bf920e72ca796387567a.pdf#:~:text=Since%20the%20goal%20of%20detecting,objects%20invisible%20or%20almost%20invisible)[reddit.com](https://www.reddit.com/r/computervision/comments/1bi7f1x/estimating_cricket_ball_speed_and_trajectory_with/#:~:text=%E2%80%A2%20%202y%20ago), open dataset availability[github.com](https://github.com/BaseballCV/BaseballCV#:~:text=match%20at%20L460%20dataset%20with,format), model optimization benchmarks[cytron.io](https://www.cytron.io/tutorial/yolo-vs-ssd-a-detailed-comparison-for-object-detection?srsltid=AfmBOorFTdI3p7S8GjXhVBysWtPAB_A9bPKw_FKs0S9QM9ojK3IPDFjd#:~:text=Inference%20Speed%20)[researchgate.net](https://www.researchgate.net/publication/393264089_Tennis_ball_detection_based_on_YOLOv5_with_tensorrt#:~:text=practical%20edge%20devices,pt%29%20to%20the%20ONNX), and details from the existing AR strike zone implementation, to ensure a comprehensive and well-grounded report.출처: 이 분석은 YOLOv8n과 기타 모델들의 성능 연구(7universum.com7universum.com), 스포츠 공 추적에 대한 실무 사례(YOLOmedium.commedium.com), 소형 객체를 위한 데이터 증강 모범 사례(pdfs.semanticscholar.orgreddit.com), 공개 데이터셋 가용성(github.com), 모델 최적화 벤치마크(cytron.ioresearchgate.net), 및 기존 AR 스트라이크 존 구현에 대한 세부 정보를 기반으로 하여 포괄적이고 타당한 보고서를 보장합니다.