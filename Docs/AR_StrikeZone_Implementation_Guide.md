# ğŸ¯ AR Strike Zone ë”¥ëŸ¬ë‹ ì—…ê·¸ë ˆì´ë“œ - ì™„ì „ êµ¬í˜„ ê°€ì´ë“œ

> **ëª©í‘œ**: ì‹¤ì œ í°ìƒ‰ ì•¼êµ¬ê³µ | 60fps 1080p | íœ´ëŒ€í° ì˜¨ë””ë°”ì´ìŠ¤ ì¶”ë¡  + ì‹¤ì‹œê°„ ìŒì„± | ì„œë²„ ì „ì†¡ + ì›¹ ì‹œê°í™”

---

## ğŸ“‹ ëª©ì°¨

1. [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”](#1-ì‹œìŠ¤í…œ-ì•„í‚¤í…ì²˜-ê°œìš”)
2. [Phase 0: ë°ì´í„° ìˆ˜ì§‘ ë° ë²¤ì¹˜ë§ˆí¬](#2-phase-0-ë°ì´í„°-ìˆ˜ì§‘-ë°-ë²¤ì¹˜ë§ˆí¬-1-2ì£¼)
3. [Phase 1: ë”¥ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ](#3-phase-1-ë”¥ëŸ¬ë‹-ëª¨ë¸-ê°œë°œ-2-3ì£¼)
4. [Phase 2: ì•ˆë“œë¡œì´ë“œ ì•± ê°œë°œ](#4-phase-2-ì•ˆë“œë¡œì´ë“œ-ì•±-ê°œë°œ-3-4ì£¼)
5. [Phase 3: ì„œë²„ ë° ì›¹ ëŒ€ì‹œë³´ë“œ](#5-phase-3-ì„œë²„-ë°-ì›¹-ëŒ€ì‹œë³´ë“œ-3-4ì£¼)
6. [Phase 4: ë¬¼ë¦¬ ê¸°ë°˜ íë£¨í”„ ì¶”ì ](#6-phase-4-ë¬¼ë¦¬-ê¸°ë°˜-íë£¨í”„-ì¶”ì -2-3ì£¼)
7. [Phase 5: ê³ ê¸‰ ë°ì´í„° ì¦ê°•](#7-phase-5-ê³ ê¸‰-ë°ì´í„°-ì¦ê°•-2ì£¼)
8. [í•„ìˆ˜ ì°¸ê³  ë…¼ë¬¸ ëª©ë¡](#8-í•„ìˆ˜-ì°¸ê³ -ë…¼ë¬¸-ëª©ë¡)
9. [ê¸°ìˆ  ìŠ¤íƒ ìƒì„¸](#9-ê¸°ìˆ -ìŠ¤íƒ-ìƒì„¸)
10. [ì‹¤í—˜ ì„¤ê³„ ë° í‰ê°€](#10-ì‹¤í—˜-ì„¤ê³„-ë°-í‰ê°€)

---

## 1. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ê°œìš”

### 1.1 ì „ì²´ ì‹œìŠ¤í…œ íë¦„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ğŸ“± Android App (On-Device)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ CameraX  â”‚â”€â”€â”€â–¶â”‚  YOLOv8n â”‚â”€â”€â”€â–¶â”‚ 3D ì¢Œí‘œ  â”‚â”€â”€â”€â–¶â”‚  íŒì •    â”‚          â”‚
â”‚  â”‚ 60fps    â”‚    â”‚ TFLite   â”‚    â”‚ ë³€í™˜     â”‚    â”‚ ë¡œì§    â”‚          â”‚
â”‚  â”‚ 1080p    â”‚    â”‚ INT8     â”‚    â”‚ (í•€í™€+   â”‚    â”‚ Strike/ â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ ArUco)   â”‚    â”‚ Ball    â”‚          â”‚
â”‚       â”‚               â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â”‚               â”‚                               â”‚                â”‚
â”‚       â–¼               â–¼                               â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ AR ì˜¤ë²„  â”‚    â”‚ ì¹¼ë§Œ     â”‚                   â”‚ TTS      â”‚          â”‚
â”‚  â”‚ ë ˆì´     â”‚    â”‚ í•„í„°     â”‚                   â”‚ ìŒì„±ì¶œë ¥ â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                       â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                            WebSocket (JSON)            â”‚
                                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ğŸ–¥ï¸ Backend Server                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚  â”‚ FastAPI  â”‚â”€â”€â”€â–¶â”‚ Postgres â”‚â”€â”€â”€â–¶â”‚ Redis    â”‚                          â”‚
â”‚  â”‚ WebSocketâ”‚    â”‚ Timescaleâ”‚    â”‚ Cache    â”‚                          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                            WebSocket (Real-time)       â”‚
                                                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ğŸŒ Web Dashboard (React)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ 3D ê¶¤ì   â”‚    â”‚ íˆíŠ¸ë§µ   â”‚    â”‚ êµ¬ì†     â”‚    â”‚ ê³„ì •ë³„   â”‚          â”‚
â”‚  â”‚ Three.js â”‚    â”‚ Plotly   â”‚    â”‚ í†µê³„     â”‚    â”‚ ê¸°ë¡     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 í•µì‹¬ ì„¤ê³„ ì›ì¹™

| ì›ì¹™ | ì„¤ëª… | êµ¬í˜„ ë°©í–¥ |
|------|------|----------|
| **ì˜¨ë””ë°”ì´ìŠ¤ ìš°ì„ ** | ëª¨ë“  ì¶”ë¡ ì€ íœ´ëŒ€í°ì—ì„œ ì‹¤í–‰ | TFLite INT8 + GPU delegate |
| **ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©** | ArUco + í•€í™€ ê¹Šì´ ì¶”ì • ìœ ì§€ | ê²€ì¶œë¶€ë§Œ ë”¥ëŸ¬ë‹ìœ¼ë¡œ êµì²´ |
| **ì‹¤ì‹œê°„ í”¼ë“œë°±** | í˜„ì¥ì—ì„œ ì¦‰ê°ì ì¸ íŒì • | ì˜¤í”„ë¼ì¸ TTS + ì €ì§€ì—° ì¶”ë¡  |
| **ë¶„ì„ì€ ì„œë²„ì—ì„œ** | ìƒì„¸ ì‹œê°í™”/í†µê³„ëŠ” ì›¹ | ê²°ê³¼ JSONë§Œ ì „ì†¡ |

### 1.3 ì§€ì—° ì‹œê°„ ëª©í‘œ (End-to-End)

```
ìº¡ì²˜ â†’ ì „ì²˜ë¦¬ â†’ ì¶”ë¡  â†’ í›„ì²˜ë¦¬ â†’ íŒì • â†’ TTS ì¶œë ¥
 5ms     10ms     30ms    5ms      1ms     50ms
                                          â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                    ì´ ëª©í‘œ: < 150ms
```

---

## 2. Phase 0: ë°ì´í„° ìˆ˜ì§‘ ë° ë²¤ì¹˜ë§ˆí¬ (1-2ì£¼)

### 2.1 ë°ì´í„° ìˆ˜ì§‘ ìš”êµ¬ì‚¬í•­

#### 2.1.1 ì´¬ì˜ í™˜ê²½ ì²´í¬ë¦¬ìŠ¤íŠ¸

```markdown
## í•„ìˆ˜ ì´¬ì˜ í™˜ê²½ (ê° í™˜ê²½ì—ì„œ ìµœì†Œ 30êµ¬ ì´ìƒ)

### ì¡°ëª… ì¡°ê±´
- [ ] ë§‘ì€ ë‚® (ì§ì‚¬ê´‘ì„ )
- [ ] íë¦° ë‚® (í™•ì‚°ê´‘)
- [ ] ì—­ê´‘ (íƒœì–‘ì´ ì¹´ë©”ë¼ ë’¤)
- [ ] ì•¼ê°„ (ì¡°ëª…ë“± ì•„ë˜)
- [ ] ì‹¤ë‚´ (í˜•ê´‘ë“±/LED)

### ë°°ê²½ ì¡°ê±´
- [ ] ê¹¨ë—í•œ ë…¹ìƒ‰ ì”ë”” ë°°ê²½
- [ ] í°ìƒ‰ ìœ ë‹ˆí¼ ì°©ìš© íˆ¬ìˆ˜
- [ ] ê´€ì¤‘ì„/ê´‘ê³ íŒ í¬í•¨
- [ ] í¬ìˆ˜ ì¥ë¹„ í¬í•¨
- [ ] ë¹ˆ í”„ë ˆì„ (ê³µ ì—†ìŒ) - ìµœì†Œ 100í”„ë ˆì„

### íˆ¬êµ¬ ì¢…ë¥˜
- [ ] ì§êµ¬ (120-150 km/h)
- [ ] ë³€í™”êµ¬ (ì»¤ë¸Œ, ìŠ¬ë¼ì´ë”)
- [ ] ì²´ì¸ì§€ì—… (ëŠë¦° ê³µ)
```

#### 2.1.2 ì¹´ë©”ë¼ ì„¤ì • ìŠ¤í™

```yaml
camera_settings:
  resolution: 1920x1080
  frame_rate: 60fps
  codec: H.264 ë˜ëŠ” H.265
  bitrate: 50Mbps ì´ìƒ
  
positioning:
  distance_from_plate: 18.44m (ë§ˆìš´ë“œ ê±°ë¦¬) ë˜ëŠ” ê·¸ ë’¤
  height: 1.2m - 1.5m (í¬ìˆ˜ ì‹œì  ê·¼ì‚¬)
  angle: ì •ë©´ ë˜ëŠ” ì•½ê°„ ì¸¡ë©´ (Â±15Â°)
  
aruco_marker:
  size: 20cm x 20cm
  dictionary: DICT_6X6_250
  placement: í™ˆí”Œë ˆì´íŠ¸ ì˜† ë˜ëŠ” ì•
```

### 2.2 ë¼ë²¨ë§ ê·œì¹™ ìƒì„¸

#### 2.2.1 YOLO í˜•ì‹ ë¼ë²¨ë§

```
# ë¼ë²¨ íŒŒì¼ í˜•ì‹: frame_000001.txt
# class_id  x_center  y_center  width  height
# (ëª¨ë‘ 0-1ë¡œ ì •ê·œí™”)

0 0.5234 0.4521 0.0156 0.0278
```

#### 2.2.2 ë¼ë²¨ë§ ë„êµ¬ ì¶”ì²œ

| ë„êµ¬ | íŠ¹ì§• | ì¶”ì²œ ìš©ë„ |
|------|------|----------|
| **CVAT** | ì›¹ ê¸°ë°˜, ë¹„ë””ì˜¤ ì§€ì›, ì¸í„°í´ë ˆì´ì…˜ | ëŒ€ëŸ‰ ë¹„ë””ì˜¤ ë¼ë²¨ë§ |
| **LabelImg** | ê°€ë³ê³  ë¹ ë¦„, YOLO í˜•ì‹ ì§ì ‘ ì§€ì› | ë¹ ë¥¸ ì´ë¯¸ì§€ ë¼ë²¨ë§ |
| **Roboflow** | ìë™ ì¦ê°•, ë°ì´í„°ì…‹ ê´€ë¦¬ | íŒ€ í˜‘ì—…, ë²„ì „ ê´€ë¦¬ |

#### 2.2.3 ë¼ë²¨ë§ ê°€ì´ë“œë¼ì¸

```markdown
## ê³µ ë¼ë²¨ë§ ê·œì¹™

1. **ë°”ìš´ë”© ë°•ìŠ¤ í¬ê¸°**
   - ê³µ ì™¸ê³½ì„ ê½‰ ì±„ìš°ëŠ” ì •ì‚¬ê°í˜•ì— ê°€ê¹ê²Œ
   - ëª¨ì…˜ ë¸”ëŸ¬ê°€ ìˆì–´ë„ ê³µì˜ "í•µì‹¬ ì˜ì—­"ë§Œ í¬í•¨
   - ë¸”ëŸ¬ ê¼¬ë¦¬ëŠ” ì œì™¸

2. **ëª¨í˜¸í•œ ìƒí™© ì²˜ë¦¬**
   - ê³µì´ ì™„ì „íˆ ë³´ì´ì§€ ì•Šìœ¼ë©´ â†’ ë¼ë²¨ë§ ì•ˆ í•¨
   - 50% ì´ìƒ ê°€ë ¤ì§€ë©´ â†’ ë¼ë²¨ë§ ì•ˆ í•¨
   - ëª¨ì…˜ ë¸”ëŸ¬ë¡œ ì¸í•´ ì›í˜•ì´ ì•„ë‹ˆë©´ â†’ ë¸”ëŸ¬ ì¤‘ì‹¬ë¶€ë§Œ

3. **í•˜ë“œ ë„¤ê±°í‹°ë¸Œ (ê³µ ì—†ëŠ” í”„ë ˆì„)**
   - ë¹ˆ ë¼ë²¨ íŒŒì¼ ìƒì„± (0ë°”ì´íŠ¸ .txt)
   - í°ìƒ‰ ìœ ì‚¬ ê°ì²´ í¬í•¨ í”„ë ˆì„ í•„ìˆ˜
```

### 2.3 ë²¤ì¹˜ë§ˆí¬ ì§€í‘œ ì •ì˜

#### 2.3.1 ê²€ì¶œ ì„±ëŠ¥ ì§€í‘œ

```python
# í‰ê°€ ì§€í‘œ ê³„ì‚° ì˜ˆì‹œ
from collections import defaultdict

class DetectionMetrics:
    """
    ì†Œí˜• ê°ì²´(ì•¼êµ¬ê³µ) ê²€ì¶œ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
    """
    
    def __init__(self, iou_threshold=0.5):
        self.iou_threshold = iou_threshold
        self.results = defaultdict(list)
    
    def calculate_iou(self, box1, box2):
        """IoU (Intersection over Union) ê³„ì‚°"""
        x1 = max(box1[0], box2[0])
        y1 = max(box1[1], box2[1])
        x2 = min(box1[2], box2[2])
        y2 = min(box1[3], box2[3])
        
        intersection = max(0, x2 - x1) * max(0, y2 - y1)
        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0
    
    def evaluate(self, predictions, ground_truths):
        """
        Returns:
            precision: TP / (TP + FP)
            recall: TP / (TP + FN)
            mAP: mean Average Precision
        """
        # êµ¬í˜„ ìƒì„¸...
        pass

# ëª©í‘œ ì„±ëŠ¥
TARGET_METRICS = {
    'mAP@0.5': 0.85,           # 85% ì´ìƒ
    'Recall': 0.90,            # 90% ì´ìƒ (ë†“ì¹˜ë©´ ì•ˆ ë¨)
    'Precision': 0.80,         # 80% ì´ìƒ
    'mAP_small': 0.75,         # ì†Œí˜• ê°ì²´(32x32 ë¯¸ë§Œ) 75% ì´ìƒ
}
```

#### 2.3.2 ì¶”ì  ì„±ëŠ¥ ì§€í‘œ

```python
class TrackingMetrics:
    """
    ê¶¤ì  ì¶”ì  ì„±ëŠ¥ í‰ê°€ ì§€í‘œ
    """
    
    def calculate_ade(self, pred_trajectory, gt_trajectory):
        """
        ADE (Average Displacement Error)
        - ì˜ˆì¸¡ ê¶¤ì ê³¼ ì‹¤ì œ ê¶¤ì  ì‚¬ì´ì˜ í‰ê·  ê±°ë¦¬
        """
        errors = []
        for pred, gt in zip(pred_trajectory, gt_trajectory):
            dist = np.sqrt((pred[0]-gt[0])**2 + (pred[1]-gt[1])**2 + (pred[2]-gt[2])**2)
            errors.append(dist)
        return np.mean(errors)
    
    def calculate_track_retention(self, detections, total_frames):
        """
        íŠ¸ë™ ìœ ì§€ìœ¨: íˆ¬êµ¬ êµ¬ê°„ì—ì„œ ê³µì„ ê²€ì¶œí•œ í”„ë ˆì„ ë¹„ìœ¨
        """
        return len(detections) / total_frames

# ëª©í‘œ ì„±ëŠ¥
TARGET_TRACKING = {
    'ADE': 5.0,                 # 5cm ì´í•˜
    'Track_Retention': 0.85,   # 85% ì´ìƒ
    'ID_Switches': 0,          # ë‹¨ì¼ ê°ì²´ì´ë¯€ë¡œ 0
}
```

#### 2.3.3 ì‹œìŠ¤í…œ ì§€ì—° ì§€í‘œ

```python
class LatencyMetrics:
    """
    ì‹œìŠ¤í…œ ì§€ì—° ì‹œê°„ ì¸¡ì •
    """
    
    def measure_pipeline_latency(self):
        """
        ê° ë‹¨ê³„ë³„ ì§€ì—° ì‹œê°„ ì¸¡ì • (ms)
        """
        return {
            'capture': 0,           # í”„ë ˆì„ ìº¡ì²˜
            'preprocess': 0,        # ë¦¬ì‚¬ì´ì¦ˆ, ì •ê·œí™”
            'inference': 0,         # ëª¨ë¸ ì¶”ë¡ 
            'postprocess': 0,       # NMS, ì¢Œí‘œ ë³€í™˜
            'judgment': 0,          # íŒì • ë¡œì§
            'tts': 0,               # ìŒì„± ì¶œë ¥
            'total': 0              # ì „ì²´
        }

# ëª©í‘œ ì§€ì—°
TARGET_LATENCY = {
    'inference': 35,        # 35ms ì´í•˜ (â‰ˆ28fps)
    'total': 150,           # 150ms ì´í•˜ (ì²´ê° ì‹¤ì‹œê°„)
    'tts_queue': 100,       # TTS íì‰ 100ms ì´í•˜
}
```

### 2.4 ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
dataset/
â”œâ”€â”€ raw/                          # ì›ë³¸ ì˜ìƒ
â”‚   â”œâ”€â”€ day_sunny/
â”‚   â”œâ”€â”€ day_cloudy/
â”‚   â”œâ”€â”€ night/
â”‚   â””â”€â”€ indoor/
â”‚
â”œâ”€â”€ frames/                       # ì¶”ì¶œëœ í”„ë ˆì„
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â””â”€â”€ test/
â”‚   â””â”€â”€ labels/
â”‚       â”œâ”€â”€ train/
â”‚       â”œâ”€â”€ val/
â”‚       â””â”€â”€ test/
â”‚
â”œâ”€â”€ augmented/                    # ì¦ê°•ëœ ë°ì´í„°
â”‚   â”œâ”€â”€ motion_blur/
â”‚   â”œâ”€â”€ exposure/
â”‚   â””â”€â”€ background_swap/
â”‚
â”œâ”€â”€ metadata/
â”‚   â”œâ”€â”€ pitch_info.json          # íˆ¬êµ¬ ì •ë³´ (êµ¬ì†, êµ¬ì¢… ë“±)
â”‚   â””â”€â”€ camera_calibration.json  # ì¹´ë©”ë¼ ìº˜ë¦¬ë¸Œë ˆì´ì…˜
â”‚
â””â”€â”€ benchmarks/
    â”œâ”€â”€ detection_results.json
    â”œâ”€â”€ tracking_results.json
    â””â”€â”€ latency_results.json
```

---

## 3. Phase 1: ë”¥ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ (2-3ì£¼)

### 3.1 ëª¨ë¸ ì„ íƒ ê·¼ê±°

#### 3.1.1 ëª¨ë¸ ë¹„êµí‘œ

| ëª¨ë¸ | í¬ê¸° (MB) | mAP@0.5 | ëª¨ë°”ì¼ FPS | ì†Œí˜• ê°ì²´ ì„±ëŠ¥ |
|------|----------|---------|-----------|---------------|
| **YOLOv8n** | 6.2 | 37.3 | 25-35 | â­â­â­â­ |
| YOLOv8s | 22.5 | 44.9 | 15-20 | â­â­â­â­â­ |
| MobileNet-SSD | 5.8 | 20-25 | 40-50 | â­â­ |
| EfficientDet-Lite0 | 4.4 | 25.7 | 20-30 | â­â­â­ |
| NanoDet-Plus | 4.7 | 30.4 | 30-40 | â­â­â­ |

> **ì„ íƒ: YOLOv8n** - ì†Œí˜• ê°ì²´ ì„±ëŠ¥ê³¼ ì†ë„ì˜ ê· í˜•ì´ ê°€ì¥ ì¢‹ìŒ

#### 3.1.2 YOLOv8n ì•„í‚¤í…ì²˜ íŠ¹ì§•

```
YOLOv8n êµ¬ì¡°:
â”œâ”€â”€ Backbone: CSPDarknet (ê²½ëŸ‰í™”)
â”‚   â””â”€â”€ P3, P4, P5 í”¼ì²˜ ì¶”ì¶œ
â”œâ”€â”€ Neck: PAFPN (ì–‘ë°©í–¥ í”¼ì²˜ ìœµí•©)
â”‚   â””â”€â”€ ë©€í‹°ìŠ¤ì¼€ì¼ í”¼ì²˜ ê²°í•©
â””â”€â”€ Head: Decoupled Head
    â””â”€â”€ ë¶„ë¥˜/íšŒê·€ ë¶„ë¦¬ â†’ ì†Œí˜• ê°ì²´ì— ìœ ë¦¬

í•µì‹¬ ì¥ì :
1. Anchor-free ì„¤ê³„ â†’ ì‘ì€ ê°ì²´ ê²€ì¶œ ìœ ì—°ì„±
2. PAFPN â†’ ê³ í•´ìƒë„ í”¼ì²˜ ë³´ì¡´
3. 6MB ê²½ëŸ‰ â†’ ëª¨ë°”ì¼ ìµœì í™”
```

### 3.2 í•™ìŠµ í™˜ê²½ ì„¤ì •

#### 3.2.1 í™˜ê²½ ì„¤ì • ì½”ë“œ

```python
# train_config.py
from ultralytics import YOLO

# ë°ì´í„°ì…‹ ì„¤ì • (data.yaml)
DATA_YAML = """
path: ./dataset
train: frames/images/train
val: frames/images/val
test: frames/images/test

nc: 1  # í´ë˜ìŠ¤ ìˆ˜ (baseballë§Œ)
names: ['baseball']
"""

# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
TRAIN_CONFIG = {
    'model': 'yolov8n.pt',          # ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜
    'data': 'data.yaml',
    'epochs': 300,
    'imgsz': 416,                    # ì…ë ¥ í¬ê¸°
    'batch': 16,
    'device': 0,                     # GPU
    
    # ì†Œí˜• ê°ì²´ ìµœì í™”
    'lr0': 0.01,
    'lrf': 0.01,
    'momentum': 0.937,
    'weight_decay': 0.0005,
    
    # ì¦ê°• (ì´ˆê¸°ì—ëŠ” ë³´ìˆ˜ì ìœ¼ë¡œ)
    'hsv_h': 0.015,
    'hsv_s': 0.7,
    'hsv_v': 0.4,
    'degrees': 0,                    # íšŒì „ ì•ˆ í•¨ (ê³µì€ êµ¬í˜•)
    'translate': 0.1,
    'scale': 0.5,
    'flipud': 0.0,                   # ìƒí•˜ë°˜ì „ ì•ˆ í•¨
    'fliplr': 0.5,                   # ì¢Œìš°ë°˜ì „ í—ˆìš©
    'mosaic': 1.0,
    'mixup': 0.1,
}

def train():
    model = YOLO('yolov8n.pt')
    results = model.train(**TRAIN_CONFIG)
    return results
```

#### 3.2.2 ì†Œí˜• ê°ì²´ íŠ¹í™” ìˆ˜ì • (ì„ íƒì )

```python
# ê³ í•´ìƒë„ í”¼ì²˜ í—¤ë“œ ì¶”ê°€ (P2 ë ˆë²¨)
# ultralytics/nn/modules/head.py ìˆ˜ì •

"""
ê¸°ë³¸ YOLOv8: P3(80x80), P4(40x40), P5(20x20)
ìˆ˜ì • ë²„ì „: P2(160x160) ì¶”ê°€ â†’ ì‘ì€ ê³µ ê²€ì¶œ í–¥ìƒ

íš¨ê³¼: 10-15px í¬ê¸° ê°ì²´ ê²€ì¶œë¥  ì•½ 15% í–¥ìƒ
ë¹„ìš©: ì¶”ë¡  ì‹œê°„ ì•½ 20% ì¦ê°€
"""

# ë˜ëŠ” ì…ë ¥ í•´ìƒë„ ì¦ê°€ë¡œ ëŒ€ì²´ (ë” ê°„ë‹¨)
# imgsz: 416 â†’ 640 (ì¶”ë¡  ì‹œê°„ 2ë°° ì¦ê°€ ì£¼ì˜)
```

### 3.3 TFLite ë³€í™˜ ë° ìµœì í™”

#### 3.3.1 ë³€í™˜ íŒŒì´í”„ë¼ì¸

```python
# export_tflite.py
from ultralytics import YOLO
import tensorflow as tf

def export_to_tflite(model_path, output_path, quantize=True):
    """
    YOLOv8 â†’ ONNX â†’ TFLite ë³€í™˜
    """
    # 1. YOLO ëª¨ë¸ ë¡œë“œ
    model = YOLO(model_path)
    
    # 2. TFLiteë¡œ ì§ì ‘ ë‚´ë³´ë‚´ê¸° (Ultralytics ì§€ì›)
    model.export(
        format='tflite',
        imgsz=416,
        half=False,           # FP16 ë¹„í™œì„±í™” (INT8 ì‚¬ìš© ì‹œ)
        int8=quantize,        # INT8 ì–‘ìí™”
        data='data.yaml',     # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ìš© ë°ì´í„°
    )
    
    print(f"Exported to: {output_path}")

def create_representative_dataset():
    """
    INT8 ì–‘ìí™”ë¥¼ ìœ„í•œ ëŒ€í‘œ ë°ì´í„°ì…‹ ìƒì„±
    - ìµœì†Œ 100ê°œ ì´ìƒì˜ ëŒ€í‘œ ì´ë¯¸ì§€ í•„ìš”
    """
    import glob
    import cv2
    import numpy as np
    
    image_paths = glob.glob('dataset/frames/images/train/*.jpg')[:200]
    
    def representative_data_gen():
        for path in image_paths:
            img = cv2.imread(path)
            img = cv2.resize(img, (416, 416))
            img = img.astype(np.float32) / 255.0
            img = np.expand_dims(img, axis=0)
            yield [img]
    
    return representative_data_gen

# ë³€í™˜ ì‹¤í–‰
export_to_tflite('runs/detect/train/weights/best.pt', 'baseball_detector.tflite')
```

#### 3.3.2 ëª¨ë¸ í¬ê¸° ë° ì„±ëŠ¥ ë¹„êµ

```
ë³€í™˜ ê²°ê³¼ ë¹„êµ:

| í˜•ì‹           | í¬ê¸°    | ì¶”ë¡  ì‹œê°„ (Pixel 6) | mAP ë³€í™” |
|---------------|---------|-------------------|----------|
| PyTorch (FP32)| 6.2 MB  | -                 | ê¸°ì¤€     |
| ONNX (FP32)   | 12.4 MB | 45ms              | 0%       |
| TFLite (FP32) | 12.4 MB | 40ms              | 0%       |
| TFLite (FP16) | 6.2 MB  | 32ms              | -0.1%    |
| TFLite (INT8) | 3.2 MB  | 25ms              | -1~2%    |

â†’ INT8 ê¶Œì¥: í¬ê¸° ì ˆë°˜, ì†ë„ 1.6ë°°, ì •í™•ë„ ì†ì‹¤ ë¯¸ë¯¸
```

### 3.4 ê²€ì¦ ë° í…ŒìŠ¤íŠ¸

#### 3.4.1 TFLite ëª¨ë¸ ê²€ì¦ ì½”ë“œ

```python
# validate_tflite.py
import numpy as np
import tensorflow as tf
import cv2
import time

class TFLiteDetector:
    def __init__(self, model_path):
        self.interpreter = tf.lite.Interpreter(model_path=model_path)
        self.interpreter.allocate_tensors()
        
        self.input_details = self.interpreter.get_input_details()
        self.output_details = self.interpreter.get_output_details()
        
        self.input_shape = self.input_details[0]['shape'][1:3]
    
    def preprocess(self, image):
        """ì…ë ¥ ì´ë¯¸ì§€ ì „ì²˜ë¦¬"""
        img = cv2.resize(image, tuple(self.input_shape))
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = img.astype(np.float32) / 255.0
        img = np.expand_dims(img, axis=0)
        return img
    
    def detect(self, image, conf_threshold=0.5):
        """ê°ì²´ ê²€ì¶œ ìˆ˜í–‰"""
        # ì „ì²˜ë¦¬
        input_data = self.preprocess(image)
        
        # ì¶”ë¡ 
        start_time = time.time()
        self.interpreter.set_tensor(self.input_details[0]['index'], input_data)
        self.interpreter.invoke()
        inference_time = (time.time() - start_time) * 1000
        
        # ì¶œë ¥ íŒŒì‹± (YOLOv8 ì¶œë ¥ í˜•ì‹)
        output = self.interpreter.get_tensor(self.output_details[0]['index'])
        
        # NMS ë° í›„ì²˜ë¦¬
        detections = self.postprocess(output, conf_threshold)
        
        return detections, inference_time
    
    def postprocess(self, output, conf_threshold):
        """YOLOv8 ì¶œë ¥ í›„ì²˜ë¦¬"""
        # output shape: [1, 84, 8400] for YOLOv8n
        # 84 = 4 (bbox) + 80 (classes) â†’ ìš°ë¦¬ëŠ” 1 classë§Œ ì‚¬ìš©
        
        predictions = output[0].T  # [8400, 84]
        
        # ì‹ ë¢°ë„ í•„í„°ë§
        scores = predictions[:, 4]  # objectness score
        mask = scores > conf_threshold
        
        boxes = predictions[mask, :4]
        scores = scores[mask]
        
        # NMS ì ìš©
        indices = cv2.dnn.NMSBoxes(
            boxes.tolist(), scores.tolist(),
            conf_threshold, 0.45
        )
        
        return [(boxes[i], scores[i]) for i in indices.flatten()]

# ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
def benchmark_model(model_path, test_images, num_runs=100):
    detector = TFLiteDetector(model_path)
    
    latencies = []
    for img_path in test_images[:num_runs]:
        img = cv2.imread(img_path)
        _, latency = detector.detect(img)
        latencies.append(latency)
    
    print(f"Average Inference Time: {np.mean(latencies):.2f}ms")
    print(f"Std: {np.std(latencies):.2f}ms")
    print(f"Min: {np.min(latencies):.2f}ms")
    print(f"Max: {np.max(latencies):.2f}ms")
    print(f"FPS: {1000/np.mean(latencies):.1f}")
```

---

## 4. Phase 2: ì•ˆë“œë¡œì´ë“œ ì•± ê°œë°œ (3-4ì£¼)

### 4.1 í”„ë¡œì íŠ¸ êµ¬ì¡°

```
app/
â”œâ”€â”€ src/main/
â”‚   â”œâ”€â”€ java/com/strikezone/
â”‚   â”‚   â”œâ”€â”€ MainActivity.kt
â”‚   â”‚   â”œâ”€â”€ camera/
â”‚   â”‚   â”‚   â”œâ”€â”€ CameraManager.kt
â”‚   â”‚   â”‚   â””â”€â”€ FrameAnalyzer.kt
â”‚   â”‚   â”œâ”€â”€ detection/
â”‚   â”‚   â”‚   â”œâ”€â”€ BallDetector.kt
â”‚   â”‚   â”‚   â””â”€â”€ TFLiteWrapper.kt
â”‚   â”‚   â”œâ”€â”€ tracking/
â”‚   â”‚   â”‚   â”œâ”€â”€ KalmanTracker.kt
â”‚   â”‚   â”‚   â””â”€â”€ PhysicsModel.kt
â”‚   â”‚   â”œâ”€â”€ judgment/
â”‚   â”‚   â”‚   â”œâ”€â”€ StrikeZone.kt
â”‚   â”‚   â”‚   â””â”€â”€ PitchJudgment.kt
â”‚   â”‚   â”œâ”€â”€ ar/
â”‚   â”‚   â”‚   â”œâ”€â”€ ArUcoDetector.kt
â”‚   â”‚   â”‚   â””â”€â”€ AROverlay.kt
â”‚   â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”‚   â””â”€â”€ TTSManager.kt
â”‚   â”‚   â””â”€â”€ network/
â”‚   â”‚       â””â”€â”€ WebSocketClient.kt
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â””â”€â”€ baseball_detector.tflite
â”‚   â””â”€â”€ res/
â”‚       â””â”€â”€ ...
â””â”€â”€ build.gradle.kts
```

### 4.2 í•µì‹¬ ì»´í¬ë„ŒíŠ¸ êµ¬í˜„

#### 4.2.1 CameraX ì„¤ì • (60fps 1080p)

```kotlin
// CameraManager.kt
class CameraManager(
    private val context: Context,
    private val lifecycleOwner: LifecycleOwner,
    private val analyzer: ImageAnalysis.Analyzer
) {
    private lateinit var cameraProvider: ProcessCameraProvider
    
    fun startCamera(previewView: PreviewView) {
        val cameraProviderFuture = ProcessCameraProvider.getInstance(context)
        
        cameraProviderFuture.addListener({
            cameraProvider = cameraProviderFuture.get()
            
            // Preview ì„¤ì •
            val preview = Preview.Builder()
                .setTargetResolution(Size(1920, 1080))
                .build()
                .also { it.setSurfaceProvider(previewView.surfaceProvider) }
            
            // ImageAnalysis ì„¤ì • (60fps ëª©í‘œ)
            val imageAnalysis = ImageAnalysis.Builder()
                .setTargetResolution(Size(1920, 1080))
                .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
                .setOutputImageFormat(ImageAnalysis.OUTPUT_IMAGE_FORMAT_RGBA_8888)
                .build()
                .also { it.setAnalyzer(ContextCompat.getMainExecutor(context), analyzer) }
            
            // ì¹´ë©”ë¼ ë°”ì¸ë”©
            val cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA
            
            try {
                cameraProvider.unbindAll()
                val camera = cameraProvider.bindToLifecycle(
                    lifecycleOwner,
                    cameraSelector,
                    preview,
                    imageAnalysis
                )
                
                // ìˆ˜ë™ ë…¸ì¶œ/í¬ì»¤ìŠ¤ ì„¤ì • (ì„ íƒì )
                setupCameraControls(camera)
                
            } catch (e: Exception) {
                Log.e("CameraManager", "Camera binding failed", e)
            }
        }, ContextCompat.getMainExecutor(context))
    }
    
    private fun setupCameraControls(camera: Camera) {
        // ê³ ì† ì…”í„° ì„¤ì • (ëª¨ì…˜ ë¸”ëŸ¬ ê°ì†Œ)
        camera.cameraControl.setExposureCompensationIndex(-1)
    }
}
```

#### 4.2.2 TFLite ì¶”ë¡  ë˜í¼

```kotlin
// TFLiteWrapper.kt
class TFLiteWrapper(context: Context, modelPath: String) {
    
    private val interpreter: Interpreter
    private val inputShape: IntArray
    private val outputShape: IntArray
    
    // GPU Delegate ì‚¬ìš©
    private val gpuDelegate: GpuDelegate?
    
    init {
        // GPU Delegate ì´ˆê¸°í™”
        gpuDelegate = try {
            GpuDelegate(GpuDelegate.Options().apply {
                setPrecisionLossAllowed(true)  // ì„±ëŠ¥ í–¥ìƒ
                setInferencePreference(GpuDelegate.Options.INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER)
            })
        } catch (e: Exception) {
            Log.w("TFLite", "GPU Delegate not available, falling back to CPU")
            null
        }
        
        // Interpreter ì˜µì…˜
        val options = Interpreter.Options().apply {
            setNumThreads(4)
            gpuDelegate?.let { addDelegate(it) }
        }
        
        // ëª¨ë¸ ë¡œë“œ
        val modelBuffer = loadModelFile(context, modelPath)
        interpreter = Interpreter(modelBuffer, options)
        
        // ì…ì¶œë ¥ í˜•íƒœ ì €ì¥
        inputShape = interpreter.getInputTensor(0).shape()
        outputShape = interpreter.getOutputTensor(0).shape()
        
        Log.d("TFLite", "Model loaded: input=$inputShape, output=$outputShape")
    }
    
    private fun loadModelFile(context: Context, modelPath: String): MappedByteBuffer {
        val assetFileDescriptor = context.assets.openFd(modelPath)
        val fileInputStream = FileInputStream(assetFileDescriptor.fileDescriptor)
        val fileChannel = fileInputStream.channel
        return fileChannel.map(
            FileChannel.MapMode.READ_ONLY,
            assetFileDescriptor.startOffset,
            assetFileDescriptor.declaredLength
        )
    }
    
    fun detect(bitmap: Bitmap, confThreshold: Float = 0.5f): List<Detection> {
        // ì „ì²˜ë¦¬
        val inputBuffer = preprocessBitmap(bitmap)
        
        // ì¶œë ¥ ë²„í¼ ì¤€ë¹„
        val outputBuffer = Array(1) { Array(outputShape[1]) { FloatArray(outputShape[2]) } }
        
        // ì¶”ë¡ 
        val startTime = SystemClock.elapsedRealtimeNanos()
        interpreter.run(inputBuffer, outputBuffer)
        val inferenceTime = (SystemClock.elapsedRealtimeNanos() - startTime) / 1_000_000f
        
        Log.d("TFLite", "Inference time: ${inferenceTime}ms")
        
        // í›„ì²˜ë¦¬
        return postprocess(outputBuffer[0], confThreshold, bitmap.width, bitmap.height)
    }
    
    private fun preprocessBitmap(bitmap: Bitmap): ByteBuffer {
        val inputSize = inputShape[1]  // 416
        
        // ë¦¬ì‚¬ì´ì¦ˆ
        val resized = Bitmap.createScaledBitmap(bitmap, inputSize, inputSize, true)
        
        // ByteBuffer ìƒì„± (FLOAT32)
        val buffer = ByteBuffer.allocateDirect(1 * inputSize * inputSize * 3 * 4)
        buffer.order(ByteOrder.nativeOrder())
        
        val pixels = IntArray(inputSize * inputSize)
        resized.getPixels(pixels, 0, inputSize, 0, 0, inputSize, inputSize)
        
        for (pixel in pixels) {
            // RGB ì¶”ì¶œ ë° ì •ê·œí™” (0-1)
            buffer.putFloat(((pixel shr 16) and 0xFF) / 255.0f)  // R
            buffer.putFloat(((pixel shr 8) and 0xFF) / 255.0f)   // G
            buffer.putFloat((pixel and 0xFF) / 255.0f)           // B
        }
        
        buffer.rewind()
        return buffer
    }
    
    private fun postprocess(
        output: Array<FloatArray>,
        confThreshold: Float,
        origWidth: Int,
        origHeight: Int
    ): List<Detection> {
        val detections = mutableListOf<Detection>()
        
        // YOLOv8 ì¶œë ¥: [84, 8400] â†’ transpose
        // 84 = 4 (xywh) + 80 (classes) â†’ ìš°ë¦¬ëŠ” 1 classë§Œ
        
        for (i in 0 until output[0].size) {
            val confidence = output[4][i]  // class 0 confidence
            
            if (confidence > confThreshold) {
                // ì¢Œí‘œ ë³µì› (0-1 â†’ ì›ë³¸ í¬ê¸°)
                val cx = output[0][i] * origWidth
                val cy = output[1][i] * origHeight
                val w = output[2][i] * origWidth
                val h = output[3][i] * origHeight
                
                detections.add(Detection(
                    centerX = cx,
                    centerY = cy,
                    width = w,
                    height = h,
                    confidence = confidence
                ))
            }
        }
        
        // NMS ì ìš©
        return applyNMS(detections, 0.45f)
    }
    
    private fun applyNMS(detections: List<Detection>, iouThreshold: Float): List<Detection> {
        if (detections.isEmpty()) return emptyList()
        
        val sorted = detections.sortedByDescending { it.confidence }
        val selected = mutableListOf<Detection>()
        val active = BooleanArray(sorted.size) { true }
        
        for (i in sorted.indices) {
            if (!active[i]) continue
            selected.add(sorted[i])
            
            for (j in i + 1 until sorted.size) {
                if (active[j] && calculateIoU(sorted[i], sorted[j]) > iouThreshold) {
                    active[j] = false
                }
            }
        }
        
        return selected
    }
    
    fun close() {
        interpreter.close()
        gpuDelegate?.close()
    }
}

data class Detection(
    val centerX: Float,
    val centerY: Float,
    val width: Float,
    val height: Float,
    val confidence: Float
) {
    val radius: Float get() = (width + height) / 4  // í‰ê·  ë°˜ì§€ë¦„
}
```

#### 4.2.3 3D ì¢Œí‘œ ë³€í™˜ (í•€í™€ ëª¨ë¸)

```kotlin
// CoordinateTransformer.kt
class CoordinateTransformer(
    private val cameraMatrix: FloatArray,  // 3x3 ë‚´ë¶€ íŒŒë¼ë¯¸í„°
    private val distCoeffs: FloatArray,    // ì™œê³¡ ê³„ìˆ˜
    private val ballRadiusReal: Float = 0.0365f  // ì•¼êµ¬ê³µ ë°˜ì§€ë¦„ 3.65cm
) {
    
    /**
     * 2D ê²€ì¶œ â†’ 3D ì¹´ë©”ë¼ ì¢Œí‘œ ë³€í™˜
     * 
     * @param detection ê²€ì¶œ ê²°ê³¼ (í”½ì…€ ì¢Œí‘œ)
     * @return 3D ìœ„ì¹˜ (ì¹´ë©”ë¼ ì¢Œí‘œê³„, ë¯¸í„°)
     */
    fun estimateDepth(detection: Detection): FloatArray {
        // í•€í™€ ëª¨ë¸: Z = (f * R_real) / r_pixel
        val focalLength = cameraMatrix[0]  // fx (í”½ì…€ ë‹¨ìœ„)
        
        // ê¹Šì´ ì¶”ì •
        val z = (focalLength * ballRadiusReal) / detection.radius
        
        // 2D â†’ 3D ì—­íˆ¬ì˜
        val cx = cameraMatrix[2]  // ì£¼ì  x
        val cy = cameraMatrix[5]  // ì£¼ì  y
        
        val x = (detection.centerX - cx) * z / focalLength
        val y = (detection.centerY - cy) * z / focalLength
        
        return floatArrayOf(x, y, z)
    }
    
    /**
     * ì¹´ë©”ë¼ ì¢Œí‘œ â†’ ArUco ë§ˆì»¤ ì¢Œí‘œ ë³€í™˜
     * 
     * @param point3D ì¹´ë©”ë¼ ì¢Œí‘œê³„ 3D ì 
     * @param rvec ArUco íšŒì „ ë²¡í„°
     * @param tvec ArUco ì´ë™ ë²¡í„°
     * @return ë§ˆì»¤ ì¢Œí‘œê³„ 3D ì 
     */
    fun transformToMarkerCoord(
        point3D: FloatArray,
        rvec: FloatArray,
        tvec: FloatArray
    ): FloatArray {
        // íšŒì „ í–‰ë ¬ ê³„ì‚°
        val rotMat = rodrigues(rvec)
        
        // ë³€í™˜: P_marker = R^T * (P_cam - t)
        val translated = floatArrayOf(
            point3D[0] - tvec[0],
            point3D[1] - tvec[1],
            point3D[2] - tvec[2]
        )
        
        // R^T * translated
        val result = FloatArray(3)
        for (i in 0..2) {
            result[i] = rotMat[i] * translated[0] + 
                        rotMat[3 + i] * translated[1] + 
                        rotMat[6 + i] * translated[2]
        }
        
        return result
    }
    
    private fun rodrigues(rvec: FloatArray): FloatArray {
        // OpenCVì˜ Rodrigues ê³µì‹ êµ¬í˜„
        val theta = sqrt(rvec[0]*rvec[0] + rvec[1]*rvec[1] + rvec[2]*rvec[2])
        
        if (theta < 1e-6) {
            return floatArrayOf(1f, 0f, 0f, 0f, 1f, 0f, 0f, 0f, 1f)
        }
        
        val k = floatArrayOf(rvec[0]/theta, rvec[1]/theta, rvec[2]/theta)
        val c = cos(theta)
        val s = sin(theta)
        
        // íšŒì „ í–‰ë ¬
        return floatArrayOf(
            c + k[0]*k[0]*(1-c),     k[0]*k[1]*(1-c) - k[2]*s,  k[0]*k[2]*(1-c) + k[1]*s,
            k[1]*k[0]*(1-c) + k[2]*s, c + k[1]*k[1]*(1-c),       k[1]*k[2]*(1-c) - k[0]*s,
            k[2]*k[0]*(1-c) - k[1]*s, k[2]*k[1]*(1-c) + k[0]*s,  c + k[2]*k[2]*(1-c)
        )
    }
}
```

#### 4.2.4 TTS ìŒì„± ì¶œë ¥

```kotlin
// TTSManager.kt
class TTSManager(private val context: Context) : TextToSpeech.OnInitListener {
    
    private var tts: TextToSpeech? = null
    private var isInitialized = false
    
    // ìŒì„± ì¶œë ¥ í (ì¤‘ë³µ ë°©ì§€)
    private var lastSpokenTime = 0L
    private val minSpeakInterval = 500L  // ìµœì†Œ 0.5ì´ˆ ê°„ê²©
    
    init {
        tts = TextToSpeech(context, this)
    }
    
    override fun onInit(status: Int) {
        if (status == TextToSpeech.SUCCESS) {
            // í•œêµ­ì–´ ì„¤ì •
            val result = tts?.setLanguage(Locale.KOREAN)
            
            if (result == TextToSpeech.LANG_MISSING_DATA || 
                result == TextToSpeech.LANG_NOT_SUPPORTED) {
                Log.e("TTS", "Korean language not supported")
                // ì˜ì–´ë¡œ í´ë°±
                tts?.setLanguage(Locale.US)
            }
            
            // ìŒì„± ì†ë„ ì„¤ì • (ë¹ ë¥´ê²Œ)
            tts?.setSpeechRate(1.2f)
            tts?.setPitch(1.0f)
            
            isInitialized = true
        }
    }
    
    fun speak(text: String, priority: Int = TextToSpeech.QUEUE_FLUSH) {
        if (!isInitialized) return
        
        val currentTime = System.currentTimeMillis()
        if (currentTime - lastSpokenTime < minSpeakInterval) {
            return  // ë„ˆë¬´ ë¹ ë¥¸ ì—°ì† ì¶œë ¥ ë°©ì§€
        }
        
        lastSpokenTime = currentTime
        tts?.speak(text, priority, null, "pitch_result_${currentTime}")
    }
    
    fun speakJudgment(judgment: PitchJudgment) {
        val text = when (judgment) {
            PitchJudgment.STRIKE -> "ìŠ¤íŠ¸ë¼ì´í¬"
            PitchJudgment.BALL -> "ë³¼"
            PitchJudgment.STRIKE_OUT -> "ìŠ¤íŠ¸ë¼ì´í¬ ì•„ì›ƒ"
            PitchJudgment.WALK -> "ë³¼ë„·"
        }
        speak(text)
    }
    
    fun speakSpeed(speedKmh: Float) {
        speak("${speedKmh.toInt()} í‚¬ë¡œ")
    }
    
    fun shutdown() {
        tts?.stop()
        tts?.shutdown()
    }
}

enum class PitchJudgment {
    STRIKE, BALL, STRIKE_OUT, WALK
}
```

#### 4.2.5 WebSocket í´ë¼ì´ì–¸íŠ¸

```kotlin
// WebSocketClient.kt
class WebSocketClient(
    private val serverUrl: String,
    private val onMessage: (String) -> Unit,
    private val onError: (Exception) -> Unit
) {
    private var webSocket: WebSocket? = null
    private val client = OkHttpClient.Builder()
        .pingInterval(30, TimeUnit.SECONDS)
        .build()
    
    // ì˜¤í”„ë¼ì¸ ë²„í¼
    private val offlineBuffer = mutableListOf<PitchData>()
    private var isConnected = false
    
    fun connect() {
        val request = Request.Builder()
            .url(serverUrl)
            .build()
        
        webSocket = client.newWebSocket(request, object : WebSocketListener() {
            override fun onOpen(webSocket: WebSocket, response: Response) {
                isConnected = true
                Log.d("WebSocket", "Connected to server")
                
                // ì˜¤í”„ë¼ì¸ ë²„í¼ ì „ì†¡
                flushOfflineBuffer()
            }
            
            override fun onMessage(webSocket: WebSocket, text: String) {
                onMessage(text)
            }
            
            override fun onFailure(webSocket: WebSocket, t: Throwable, response: Response?) {
                isConnected = false
                onError(t as Exception)
            }
            
            override fun onClosed(webSocket: WebSocket, code: Int, reason: String) {
                isConnected = false
            }
        })
    }
    
    fun sendPitchData(data: PitchData) {
        val json = Gson().toJson(data)
        
        if (isConnected) {
            webSocket?.send(json)
        } else {
            // ì˜¤í”„ë¼ì¸ ë²„í¼ì— ì €ì¥
            offlineBuffer.add(data)
            if (offlineBuffer.size > 1000) {
                offlineBuffer.removeAt(0)  // ì˜¤ë˜ëœ ë°ì´í„° ì œê±°
            }
        }
    }
    
    private fun flushOfflineBuffer() {
        offlineBuffer.forEach { data ->
            val json = Gson().toJson(data)
            webSocket?.send(json)
        }
        offlineBuffer.clear()
    }
    
    fun disconnect() {
        webSocket?.close(1000, "Client closing")
    }
}

data class PitchData(
    val timestamp: Long,
    val userId: String,
    val trajectory: List<Point3D>,      // 3D ê¶¤ì 
    val speed: Float,                    // km/h
    val judgment: String,                // STRIKE/BALL
    val crossingPoint: Point3D?,         // ì¡´ í†µê³¼ ìœ„ì¹˜
    val pitchType: String? = null        // êµ¬ì¢… (ì„ íƒ)
)

data class Point3D(
    val x: Float,
    val y: Float,
    val z: Float,
    val timestamp: Long
)
```

### 4.3 ì•± ê¶Œí•œ ë° ì„¤ì •

```xml
<!-- AndroidManifest.xml -->
<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    
    <!-- ê¶Œí•œ -->
    <uses-permission android:name="android.permission.CAMERA" />
    <uses-permission android:name="android.permission.INTERNET" />
    <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE" />
    
    <!-- í•˜ë“œì›¨ì–´ ìš”êµ¬ì‚¬í•­ -->
    <uses-feature android:name="android.hardware.camera" android:required="true" />
    <uses-feature android:name="android.hardware.camera.autofocus" />
    
    <application
        android:name=".StrikeZoneApp"
        android:hardwareAccelerated="true"
        android:largeHeap="true">
        
        <!-- ... -->
        
    </application>
</manifest>
```

```groovy
// build.gradle (app)
dependencies {
    // CameraX
    def camerax_version = "1.3.1"
    implementation "androidx.camera:camera-core:$camerax_version"
    implementation "androidx.camera:camera-camera2:$camerax_version"
    implementation "androidx.camera:camera-lifecycle:$camerax_version"
    implementation "androidx.camera:camera-view:$camerax_version"
    
    // TensorFlow Lite
    implementation 'org.tensorflow:tensorflow-lite:2.14.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.14.0'
    implementation 'org.tensorflow:tensorflow-lite-support:0.4.4'
    
    // OpenCV (ArUcoìš©)
    implementation 'org.opencv:opencv:4.8.0'
    
    // ë„¤íŠ¸ì›Œí¬
    implementation 'com.squareup.okhttp3:okhttp:4.12.0'
    implementation 'com.google.code.gson:gson:2.10.1'
    
    // Coroutines
    implementation 'org.jetbrains.kotlinx:kotlinx-coroutines-android:1.7.3'
}
```

---

**(ê³„ì†: Phase 3~10ì€ ë‹¤ìŒ íŒŒì¼ì—...)**

