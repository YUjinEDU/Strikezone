---
marp: true
---

아래 내용은 **“DIY(직접 제작) AR 안경”**을 구상할 때, 현실 세계 영상 + 가상 오버레이(카메라 분석)를 표시하기 위한 **기본적인 아이디어** 및 **접근 방법**을 정리한 것입니다. 아직 구체적 하드웨어나 구현 스펙이 정해지지 않은 상태라고 하셨으므로, **큰 그림** 위주로 안내드립니다.

---

## 1) 전반적 구조

### A) 광학(Optical) 방식 vs 디스플레이(See-through) 방식

1. **Optical See-Through (투명 디스플레이)**  
   - 예: Microsoft HoloLens 등에서 쓰는 “파동안내(Waveguide)”나 “미러/프리즘” 구조  
   - 현실 장면을 직접 보며, 그 위에 **투명 디스플레이**로 가상 그래픽을 겹쳐 표시  
   - 장점: 현실을 방해 없이 볼 수 있고, 레이턴시가 비교적 적음  
   - 단점: 구현이 매우 복잡하고, 광학 설계 비용이 큼(DIY 하기 어려움)

2. **Video See-Through (카메라 패스스루)**  
   - 예: VR 헤드셋처럼, 외부 카메라로 찍은 영상을 디스플레이(소형 LCD/OLED)에 보여주고, 그 위에 가상 그래픽을 합성  
   - 장점: 소프트웨어적으로 “실영상 + CG” 합성이 쉽다. Unity, OpenCV 등으로 렌더링 파이프라인을 만들기 용이  
   - 단점: “카메라+디스플레이”를 통한 시야이므로 실제 밖을 직접 보지 못하고, 레이턴시가 생길 수 있음  

**DIY 관점**에서는, “소형 모니터(디스플레이) + 카메라”로 **Video See-Through** 형식이 상대적으로 쉽습니다. 광학식 waveguide는 전문 장비·정밀도 요구가 커서 구현 난이도가 매우 높습니다.

---

## 2) 하드웨어 고려사항

1. **소형 디스플레이**  
   - 예: “근안경형” LCD/OLED 모듈(극소형 화면), 혹은 “스마트폰 디스플레이를 안경처럼 장착”  
   - 간단 예: 라즈베리파이 + 3.5인치 LCD 모듈을 한쪽 렌즈 앞에 배치, “영상”을 띄워주는 식

2. **카메라**  
   - “세계”를 촬영할 카메라(초소형 USB 카메라나 라즈베리파이 카메라 모듈)  
   - 영상 해상도, FPS, 노출 조절 등을 지원해야 함

3. **프로세싱 유닛(Compute)**  
   - “온보드”로 처리할지, “외부(PC/스마트폰)와 연결”할지 결정  
   - 예: 
     - **라즈베리파이**: 간단히 OpenCV, Python 구동 가능(속도 제약)  
     - **Jetson Nano**: GPU 있어서 딥러닝 가능  
     - **스마트폰 연동**: USB 카메라 or phone camera → 앱에서 ARCore 등

4. **전원, 무게, 폼팩터**  
   - 안경형 기기는 무게·배터리·열 발산 문제가 큼  
   - DIY로 “머리에 장착”하면 100g만 넘어도 부담이 느껴질 수 있음

---

## 3) 소프트웨어/AR 파이프라인

1. **SLAM/Tracking**  
   - 실제 “안경”을 움직일 때, “가상 객체”가 안정적으로 고정되려면 6DoF 추적(inside-out SLAM) 또는 마커 인식 필요  
   - 예: OpenVSLAM, ORB-SLAM, ArUco 마커, etc  
2. **카메라 → OpenCV/Unity/etc**  
   - 카메라로 들어온 영상을 실시간으로 처리(예: 공 위치 분석, 스트라이크 존 AR 표시),  
   - 그 결과를 디스플레이(작은 모니터)에 합성 렌더링해서 “안경에서 보이도록” 만듦
3. **렌더링 엔진**  
   - Python + OpenCV로 간단히 “실영상에 2D/3D 그래픽” 합성할 수도 있고,  
   - Unity 등 엔진을 사용해 “Video See-Through” AR 구현(“AR Foundation”은 기기 의존이 있으니 DIY에는 약간 복잡),  
   - Or “OpenGL / Vulkan” 직접 코딩 (DIY 난이도↑)

---

## 4) “분석 프로그램”과의 통합

- “야구 분석”을 한다면, “야구공 궤적, 스트라이크 존 3D” 등을 AR로 겹쳐 보이게 하고 싶을 수도 있습니다.  
- 만약 이미 PC에서 “카메라 분석(Strikezone) 코드”를 돌리고 있다면,  
  - DIY 안경에는 단순히 “결과(가상 객체 좌표)”만 전송 → 그걸 AR 렌더링하는 구조  
  - 영상은 안경 카메라 or 별도 “메인 카메라”로 촬영 → PC에서 처리 → 위치정보를 안경에 송신

---

## 5) 구체적 추천 시나리오

### 시나리오 A) “라즈베리파이 + 작은 디스플레이 + 카메라” (Video See-Through)
1. **라즈베리파이**로 카메라 입력을 받고, OpenCV로 “환경+공” 추적 + “AR 오버레이”(2D 합성)  
2. 출력은 **라즈베리파이 연결된 작은 LCD**에 띄워서, 안경 형태로 착용  
3. 문제점:  
   - Pi 성능이 낮아 FPS가 제한됨  
   - 무게·배터리·발열  
   - 그래도 DIY 구현 난이도는 waveguide보다 낮음

### 시나리오 B) **스마트폰** 기반
1. 스마트폰 장착형 “VR 헤드셋” (Google Cardboard식) → 전면 카메라 or 외부 카메라 추가  
2. ARCore(안드로이드)로 “환경 추적” + “가상 오브젝트” 렌더 → 착용자는 “카메라 패스스루”를 양안으로 본다  
3. 공 추적, 스트라이크 존 등 앱에서 구현  
4. DIY 난이도는 중간, 상업 장치(ARCore) 활용 가능, 디스플레이·배터리·CPU는 스마트폰에서 해결

### 시나리오 C) **PC + USB Cam + HMD**  
1. PC(노트북 등)에서 OpenCV/Unity/Unreal 등으로 AR 렌더링  
2. 영상은 USB Cam → PC → 합성 → VR HMD(HTC Vive, Oculus) 디스플레이(또는 DIY LCD)  
3. 무선/유선 연결로 착용, “안경보다는 좀 큰 VR 헤드셋” 형태

---

## 6) 결론 및 요약

- **DIY AR 안경** 자체를 만들려면, 크게 두 가지가 필요:
  1. **하드웨어**: 
     - (Video See-Through) 카메라, 소형 디스플레이, 프로세서(라즈베리파이 등)  
     - (Optical See-Through) waveguide/미러 → 매우 난이도↑  
  2. **소프트웨어 AR 파이프라인**: 
     - “실영상 + 가상 객체”를 정확히 합성하려면 추적(마커, SLAM)과 렌더링 필요  
     - Python+OpenCV 또는 Unity/Unreal 등 선택

- **추천 방법** (상대적으로 쉬움):
  - **“Video See-Through”** 식으로, 라즈베리파이 or Jetson Nano + 미니 LCD + 카메라 + OpenCV.  
  - 처음에는 간단 “한쪽 눈만” 작은 디스플레이로 시야에 띄우고, “공 궤적” 같은 그래픽을 합성 표시  
  - 또는, **스마트폰 기반** 헤드셋을 활용하는 게 인프라와 성능 면에서 더 편할 수도 있음  
- 야구 분석 프로그램(Strikezone)과 연동하려면, “안경에서” PC나 클라우드와 통신(예: Wi-Fi) → “현재 공 좌표” 등 받아서 AR로 시각화.  

**정리**: DIY AR 안경은 **영상 합성 + 소형 디스플레이 + 추적** 세 영역 모두 난이도가 높습니다. 그래도 “Video See-Through” 방식이 광학적인 부분을 단순화해, 구현 가능성을 높이는 편이고, **스마트폰 VR 헤드셋**을 변형해 사용하는 것도 좋은 출발점이 될 수 있습니다.  